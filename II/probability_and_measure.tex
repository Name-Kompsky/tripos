\documentclass[a4paper]{article}

\def\npart{II}

\def\ntitle{Probability and Measure}
\def\nlecturer{E.\ Breuillard}

\def\nterm{Michaelmas}
\def\nyear{2018}

\input{header}

\renewcommand{\P}{\prob} % probability measure
\DeclareMathOperator{\var}{Var} % variance

% TODO: uparrow and downarrow

\begin{document}

\input{titlepage}

\tableofcontents

\section{Lebesgue measure}

\subsection{Boolean algebra}

\begin{definition}[Boolean algebra]\index{Boolean algebra}
  Let \(X\) be a set. A \emph{Boolean algebra} on \(X\) is a family of subsets of \(X\) which
  \begin{enumerate}
  \item contains \(\emptyset\),
  \item is stable under finite unions and complementation.
  \end{enumerate}
\end{definition}

\begin{eg}\leavevmode
  \begin{itemize}
  \item The \emph{trivial Boolean algebra} \(\mathcal B = \{\emptyset, X\}\).
  \item The \emph{discrete Boolean algebra} \(\mathcal B = 2^X\), the family of all subsets of \(X\).
  \item Less trivially, if \(X\) is a topological space, the family of \emph{constructible sets} forms a Boolean algebra, where a constructible set is the finite union of locally closed set, i.e.\ a set \(E = U \cap F\) where \(U\) is open and \(F\) is closed.
  \end{itemize}
\end{eg}

\begin{definition}[finitely additive measure]\index{measure!finitely additive}
  Let \(X\) be a set and \(\mathcal B\) a Boolean algebra on \(X\). A \emph{finitely additive measure} on \((X, \mathcal B)\) is a function \(m: \mathcal B \to [0, +\infty]\) such that
  \begin{enumerate}
  \item \(m(\emptyset) = 0\),
  \item \(m(E \cup F) = m(E) + m(F)\) where \(E \cap F = \emptyset\).
  \end{enumerate}
\end{definition}

\begin{eg}\leavevmode
  \begin{enumerate}
  \item Counting measure: \(m(E) = \#E\), the cardinality of \(E\) where \(\mathcal B\) is the discrete Boolean algebra of \(X\).
  \item More generally, given \(f: X \to [0, +\infty]\), define for \(E \subseteq X\),
    \[
      m(E) = \sum_{e \in E} f(e).
    \]
  \item Suppose \(X = \coprod_{i = 1}^N X_i\), then define \(\mathcal B(X)\) to be the unions of \(X_i\)'s. Assign a weight \(a_i \geq 0\) to each \(X_i\) and define \(m(E) = \sum_{i: X_i \subseteq E} a_i\) for \(E \in \mathcal B\).
  \end{enumerate}
\end{eg}

\subsection{Jordan measure}

This section is a historic review and provides intuition for Lebesgue measure theory. We'll gloss over details of proofs in this section.

\begin{definition}
  A subset of \(\R^d\) is called \emph{elementary} if it is a finite union of \emph{boxes}, where a box is a set \(B = I_1 \times \dots I_d\) where each \(I_i\) is a finite interval of \(\R\).
\end{definition}

\begin{proposition}
  Let \(B \subseteq \R^d\) be a box. Let \(\mathcal E(B)\) be the family of elementary subsets of \(B\). Then
  \begin{enumerate}
  \item \(\mathcal E(B)\) is a Boolean algebra on \(B\),
  \item every \(E \in \mathcal E(B)\) is a disjoint finite union of boxes,
  \item if \(E \in \mathcal E(B)\) can be written as disjoint finite union in two ways, \(E = \bigcup_{i = 1}^n B_i = \bigcup_{j = 1}^m B_j'\), then \(\sum_{i = 1}^n |B_i| = \sum_{j = 1}^m |B_j'|\) where \(|B| = \prod_{i = 1}^d |b_i - a_i|\) if \(B = I_1 \times \dots \times I_d\) and \(I_i\) has endpoints \(a_i, b_i\).
  \end{enumerate}
\end{proposition}

Following this, we can define a finitely additive measure correponding to our intuition of length, area, volume etc:

\begin{proposition}
  Define \(m(E) = \sum_{i = 1}^n |B_i|\) if \(E\) is any elementary set and is the disjoint union of boxes \(B_i \subseteq \R^d\). Then \(m\) is a finitely additive measure on \(\mathcal E(B)\) for any box \(B\).
\end{proposition}

\begin{definition}
  A subset \(E \subseteq \R^d\) is \emph{Jordan measurable} if for any \(\varepsilon > 0\) there are elementary sets \(A, B\), \(A \subseteq E \subseteq B\) and \(m(B \setminus A) < \varepsilon\).
\end{definition}

\begin{remark}
  Jordan measurable sets are bounded.
\end{remark}

\begin{proposition}
  If a set \(E \subseteq \R^d\) is Jordan measurable, then
  \[
    \sup_{A \subseteq E \text{ elementary}} \{m(A)\} = \inf_{B \supseteq E \text{ elementary}} \{m(B)\}.
  \]
  In which case we define the \emph{Jordan measure} of \(E\) as
  \[
    m(E) = \sup_{A \subseteq E} \{m(A)\}.
  \]
\end{proposition}

\begin{proof}
  Take \(A_n \subseteq E\) such that \(m(A_n) \nearrow \sup\) and \(B_n \supseteq E\) such that \(m(B_n) \searrow \inf\). Note that
  \[
    \inf \leq m(B_n) = m(A_n) + m(B_n \setminus A_n) \leq \sup + m(B_n \setminus A_n) \leq \sup + \varepsilon
  \]
  for arbitrary \(\varepsilon > 0\) so they are equal.
\end{proof}

\begin{ex}\leavevmode
  \begin{enumerate}
  \item If \(B\) is a box, the family \(\mathcal J(B)\) of Jordan measurable subsets of \(B\) is a Boolean algebra.
  \item A subset \(E \subseteq [0, 1]\) is Jordan measurable if and only if \(\mathbf 1_E\), the indicator funciton on \(E\), is Riemann integrable.
  \end{enumerate}
\end{ex}

\subsection{Lebesgue measure}

Although Jordan measure corresponds to the intuition of length, area and volume, it suffer from a few severe problems and issues:
\begin{enumerate}
\item unbounded sets in \(\R^d\) are not Jordan measurable.
\item \(\mathbf 1_{\Q \cap [0, 1]}\) is not Riemann integrable, and therefore \(\Q \cap [0, 1]\) is not Jordan measurable.
\item pointwise limits of Riemann integrable functions \(f_n := \mathbf 1_{\frac{1}{n!} \Z \cap [0, 1]} \to \mathbf 1_{\Q \cap [0, 1]}\) is not Riemann integrable.
\end{enumerate}

The idea of Lebesgue is to use countable covers by boxes.

\begin{definition}
  A subset \(E \subseteq \R^d\) is \emph{Lebesgue measurable} if for all \(\varepsilon > 0\), there exists a countable union of boxes \(C\) with \(E \subseteq C\) and \(m^*(C \setminus E) < \varepsilon\), where \(m^*\), the \emph{Lebesgue outer measure}, is defined as
  \[
    m^*(E) = \inf \{\sum_{i \geq 1} |B_i|: E \subseteq \bigcup_{i \geq 1} B_i, B_i \text{ boxes}\}
  \]
  for \emph{every} subset \(E \subseteq \R^d\).
\end{definition}

\begin{remark}
  wlog in these definitions we may assume that boxes are open.
\end{remark}

% We are going to show that the family of Lebesgue measurable subsets is not only a Boolean algebra, but also stable under countable union. Next we are going to define the Lebesgue measure on the family, with the additive property (which is not possessed by Lebesgue outer measure). In fact, we can show that we cannot define a measure for \emph{all} subsets of a set.

\begin{proposition}
  \label{prop:Lebesgue measurable subset is Boolean algebra}
  The family \(\mathcal L\) of Lebesgue measurable subsets of \(\R^d\) is a Boolean algebra stable under countable unions.
\end{proposition}

\begin{lemma}\leavevmode
  \begin{enumerate}
  \item \(m^*\) is monotone: if \(E \subseteq F\) then \(m^*(E) \subseteq m^*(F)\).
  \item \(m^*\) is countably subadditive: if \(E = \bigcup_{n \geq 1} E_n\) where \(E_n \subseteq \R^d\) then
    \[
      m^*(E) \leq \sum_{n \geq 1} m^*(E_n).
    \]
  \end{enumerate}
\end{lemma}

\begin{proof}
  Monotonicity is obvious. For countable subadditivity, pick \(\varepsilon > 0\) and let \(C_n = \bigcup_{i \geq 1} C_{n, i}\) where \(C_{n, i}\) are boxes such that \(E_n \subseteq C_n\) and
  \[
    \sum_{i \geq 1} |C_{n, i}| \leq m^*(E_n) + \frac{\varepsilon}{2^n}.
  \]
  Then
  \[
    \sum_{n \geq 1} \sum_{i \geq 1} |C_{n, i}|
    \leq \sum_{n \geq 1} (m^*(E_n) + \frac{\varepsilon}{2^n})
    = \varepsilon + \sum_{n \geq 1} m^*(E_n)
  \]
  and \(E \subseteq \bigcup_{n \geq 1} C_n = \bigcup_{n \geq 1} \bigcup_{i \geq 1} C_{n, i}\) so
  \[
    m^*(E) \leq \varepsilon + \sum_{n \geq 1} m^*(E_n)
  \]
  for all \(\varepsilon > 0\).
\end{proof}

\begin{remark}
  Note that \(m^*\) is \emph{not} additive on the family of all subsets of \(\R^d\). However, it will be on \(\mathcal L\), as we will show later.
  % eg?
\end{remark}

\begin{lemma}
  If \(A, B\) are disjoint compact subsets of \(\R^d\) then
  \[
    m^*(A \cup B) = m^*(A) + m^*(B).
  \]
\end{lemma}

\begin{proof}
  \(\leq\) by the previous lemma so need to show \(\geq\). Pick \(\varepsilon > 0\). Let \(A \cup B \subseteq \bigcup_{n \geq 1} B_n\) where \(B_n\) are open boxes such that
  \[
    \sum_{n \geq 1} |B_n| \leq m^*(A \cup B) + \varepsilon.
  \]
  wlog we may assume that the side lengths of each \(B_n\) are \(< \frac{\alpha}{2}\), where
  \[
    \alpha = \inf \{\norm{x - y}_1: x \in A, y \in B\} > 0.
  \]
  where the inequality comes from the fact that \(A\) and \(B\) are compact and thus closed. wlog we may discard the \(B_n\)'s that do not interesect \(A \cup B\). Then by construction
  \[
    \sum_{n \geq 1} |B_n| = \sum_{n \geq 1, B_n \cap A = \emptyset} |B_n| + \sum_{n \geq 1, B_n \cap B = \emptyset} |B_n|
    \geq m^*(A) + m^*(B)
  \]
  so
  \[
    \varepsilon + m^*(A \cup B) \geq m^*(A) + m^*(B)
  \]
  for all \(\varepsilon\).
\end{proof}

\begin{lemma}
  If \(E \subseteq \R^d\) has \(m^*(E) = 0\) then \(E \in \mathcal L\).
\end{lemma}

\begin{definition}[null set]\index{null set}
  A set \(E \subseteq \R^d\) such that \(m^*(E) = 0\) is called a \emph{null set}.
\end{definition}

\begin{proof}
  For all \(\varepsilon > 0\), there exist \(C = \bigcup_{n \geq 1} B_n\) where \(B_n\) are boxes such that \(E \subseteq C\) and \(\sum_{n \geq 1} |B_n| \leq \varepsilon\). But
  \[
    m^*(C \setminus E) \leq m^*(C) \leq \varepsilon.
  \]
\end{proof}

\begin{lemma}
  \label{lem:open and closed sets are Lebesgue measurable}
  Every open subset of \(\R^d\) and every closed subset of \(\R^d\) is in \(\mathcal L\).
\end{lemma}

We will prove the lemma using the fact that the family of Lebesgue measurable subsets is stable under countable union, which itself \emph{does not} use this lemma. This lemma, however, will be used to show the stability under complementation. Since the proof is quite technical (it has more to do with general topology than measure theory), for brevity and fluency of ideas we present the proof the main proposition first.

\begin{proof}[Proof of \Cref{prop:Lebesgue measurable subset is Boolean algebra}]
  It is obvious that \(\emptyset \in \mathcal L\). To show it is stable under countable unions, start with \(E_n \in \mathcal L\) for \(n \geq 1\). Need to show \(E := \bigcup_{n \geq 1} E_n \in \mathcal L\).

  Pick \(\varepsilon > 0\). By assumption there exist \(C_n = \bigcup_{i \geq 1} B_{n, i}\) where \(B_{n, i}\) are boxes such that \(E_n \subseteq C_n\) and
  \[
    m^*(C_n \setminus E_n) < \frac{\varepsilon}{2^n}.
  \]
  Now
  \[
    E = \bigcup_{n \geq 1} E_n \subseteq \bigcup_{n \geq 1} C_n =: C
  \]
  so \(C\) is again a countable union of boxes and \(C \setminus E \subseteq \bigcup_{n \geq 1} C_n \setminus E_n\).
  so
  \[
    m^*(C \setminus E) \leq \sum_{n \geq 1} m^*(C_n \setminus E_n) \leq \sum_{n \geq 1} \frac{\varepsilon}{2^n} = \varepsilon
  \]
  by countable subadditivity so \(E \in \mathcal L\).

  To show it is stable under complementation, suppose \(E \in \mathcal L\). By assumption there exist \(C_n\) a countable union of boxes with \(E \subseteq C_n\) and \(m^*(C_n \setminus E) \leq \frac{1}{n}\). wlog we may assume the boxes are open so \(C_n\) is open, \(C_n^c\) is closed so \(C_n^c \in \mathcal L\). Thus \(\bigcup_{n \geq 1} C_n^c \in \mathcal L\) by first part of the proof.

  But
  \[
    m^*(E^c \setminus \bigcup_{n \geq 1} C_n^c)
    \leq m^*(E^c \setminus C_n^c)
    = m^*(C_n \setminus E)
    \leq \frac{1}{n}
  \]
  so \(m^*(E^c \setminus \bigcup_{n \geq 1} C_n^c) = 0\) so \(E^c \setminus \bigcup_{n \geq 1} C_n^c \in \mathcal L\) since it is a null set. But
  \[
    E^c = (E^c \setminus \bigcup_{n \geq 1} C_n^c) \cup \bigcup_{n \geq 1} C_n^c,
  \]
  both of which are in \(\mathcal L\) so \(E^c \in \mathcal L\).
\end{proof}

\begin{proof}[Proof of \Cref{lem:open and closed sets are Lebesgue measurable}]
  Every open set in \(\R^d\) is a countable union of boxes so is in \(\mathcal L\). It is more subtle for closed sets. %(in fact it is the key difference between this and Jordan measure).
  The key observation is that every closed set is the countable union of compact subsets so we are left to show compact sets of \(\R^d\) are in \(\mathcal L\).

  Let \(F \subseteq \R^d\) be compact. For all \(k \geq 1\), there exist \(O_k\) a countable union of open sets such that \(F \subseteq O_k := \bigcup_{i \geq 1} O_{k, i}\) where \(O_{k, i}\) are open boxes such that
  \[
    \sum_{i \geq 1} |O_{k, i}| \leq m^*(F) + \frac{1}{2^k}.
  \]
  By compactness there exist a finite subcover so we can assume \(O_k\) is a finite union of open boxes. Moreover, wlog assume that
  \begin{enumerate}
  \item the side lengths of \(O_{k, i}\) are \(\leq \frac{1}{2^k}\).
  \item for each \(i\), \(O_{k, i}\) intersects \(F\).
  \item \(O_{k + 1} \subseteq O_k\) (by replacing \(O_{k + 1}\) with \(O_{k + 1} \cap O_k\) iteratively).
  \end{enumerate}
  Then \(F = \bigcap_{k \geq 1} O_k\) and we are left to show \(m^*(O_k \setminus F) \to 0\). By additivity on disjoint compact sets,
  \[
    m^*(F) + m^*(\cl O_i \setminus O_{i + 1}) = m^*(F \cup (\cl O_i \setminus O_{i + 1}))
  \]
  so
  \[
    m^*(F) + m^*(\cl O_i \setminus O_{i + 1}) \leq m^*(\cl O_i)
    \leq \sum_{j \geq 1} |O_{i, j}|
    \leq m^*(F) + \frac{1}{2^i}
  \]
  so \(m^*(\cl O_i \setminus O_{i + 1}) \leq \frac{1}{2^i}\). Finally,
  \[
    m^*(O_k \setminus F)
    = m^*(\bigcup_{i \geq k} (O_i \setminus O_{i + 1}))
    \leq \sum_{i \geq k} m^*(O_i \setminus O_{i + 1})
    \leq \sum_{i \geq k} \frac{1}{2^i}
    = \frac{1}{2^{k - 1}}.
  \]
\end{proof}

The result we're working towards is

\begin{proposition}
  \label{prop:additivity of Lebesgue measure}
  \(m^*\) is countably additive on \(\mathcal L\), i.e.\ if \((E_n)_{n \geq 1}\) where \(E_n \in \mathcal L\) are pairwise disjoint then
  \[
    m^*(\bigcup_{n \geq 1} E_n) = \sum_{n \geq 1} m^*(E_n).
  \]
\end{proposition}

\begin{lemma}
  If \(E \in \mathcal L\) then for all \(\varepsilon > 0\) there exists \(U\) open, \(F\) closed, \(F \subseteq E \subseteq U\) such that \(m^*(U \setminus E) < \varepsilon\) and \(m^*(E \setminus F) < \varepsilon\).
\end{lemma}

\begin{proof}
  By definition of \(\mathcal L\), there exists a countable union of open boxes \(E \subseteq \bigcup_{n \geq 1} B_n\) such that \(m^*(\bigcup_{n \geq 1} B_n \setminus E) < \varepsilon\). Just take \(U = \bigcup_{n \geq 1} B_n\) which is open.

  For \(F\) do the same with \(E^c = \R^d \setminus E\) in place of \(E\).
\end{proof}

\begin{proof}[Proof of \Cref{prop:additivity of Lebesgue measure}]
  First we assume each \(E_n\) is compact. By a previous lemma \(m^*\) is additive on compact sets so for all \(N \in \N\),
  \[
    m^*(\bigcup_{n = 1}^N E_n) = \sum_{n = 1}^N m^*(E_n).
  \]
  In particular
  \[
    \sum_{n = 1}^N m^*(E_n) \leq m^*(\bigcup_{n \geq 1} E_n)
  \]
  since \(m^*\) is monotone. Take \(N \to \infty\) to get one inequality. The other direction holds by countable subadditivity of \(m^*\).

  Now assume that each \(E_n\) is a bounded subset in \(\mathcal L\). By the lemma there exists \(K_n \subseteq E_n\) closed, so compact, such that \(m^*(E_n \setminus K_n) \leq \frac{\varepsilon}{2^n}\). Since \(K_n\)'s are disjoint, by the previous case
  \[
    m^*(\bigcup_{n \geq 1} K_n) = \sum_{n \geq 1} m^*(K_n)
  \]
  then
  \begin{align*}
    &\sum_{n \geq 1} m^*(E_n) \\
    \leq& \sum_{n \geq 1} m^*(K_n) + m^*(E_n \setminus K_n) \\
    \leq& m^*(\bigcup_{n \geq 1} K_n) + \sum_{n \geq 1} \frac{\varepsilon}{2^n} \\
    \leq& m^*(\bigcup_{n \geq 1} E_n) + \varepsilon
  \end{align*}
  so one direction of inequality. Similarly the other direction holds by countable subadditivity of \(m^*\).

  For the general case, note that \(\R^d = \bigcup_{n \in \Z^d} A_n\) where \(A_n\) is bounded and in \(\mathcal L\), for example by taking \(A_n\) to be product of half open intervals of unit length. Write \(E_n\) as \(\bigcup_{m \in \Z^d} E_n \cap A_m\) so just apply the previous results to \((E_n \cap A_m)_{n\geq 1, m \in \Z^d}\).
\end{proof}

\begin{definition}[Lebesgue measure]\index{Lebesgue measure}
  \(m^*\) when restricted to \(\mathcal L\) is called the \emph{Lebesgue measure} and is simply denoted by \(m\).
\end{definition}

\begin{eg}[Vitali counterexample]
  Althought \(\mathcal L\) is pretty big (it includes all open and closed sets, countable unions and intersections of them, and has cardinality at least \(2^{\mathfrak c}\) where \(\mathfrak c\) is the continuum, by considering a compact null set with cardinality \(\mathfrak c\), and each subset thereof), it does not include every subset of \(\R^d\).
  
  Consider \((\Q, +)\), the additive subgroup of \((\R, +)\). Pick a set of representative \(E\) of the cosets of \((\Q, +)\). Choose it inside \([0, 1]\). For each \(x \in \R\), there exists a unique \(e \in E\) such that \(x - e \in \Q\) (here we require axiom of choice). Claim that \(E \notin \mathcal L\) and \(m^*\) is not additive on the family of all subsets of \(\R^d\).

  \begin{proof}
    Pick distinct rationals \(p_1, \dots, p_N\) in \([0, 1]\). The sets \(p_i + E\) are pairwise disjoint so if \(m^*\) were additive then we would have
    \[
      m^*(\bigcup_{i = 1}^N p_i + E)
      = \sum_{i = 1}^N m^*(p_i + E)
      = N \sum_{i = 1}^N m^*(E)
    \]
    by translation invariance of \(m^*\). But then
    \[
      \bigcup_{i = 1}^N p_i + E \subseteq [0, 2]
    \]
    since \(E \subseteq [0, 1]\) so by monotonicity of \(m^*\) have
    \[
      m^*(\bigcup_{i = 1}^N p_i + E) \leq 2
    \]
    so for all \(N m^*(E) \leq 2\) so \(m^*(E) = 0\). But
    \[
      [0, 1] \subseteq \bigcup_{q \in \Q} E + q = \R,
    \]
    by countable subadditivity of \(m^*\),
    \[
      1 = m^*([0, 1]) \leq \sum_{q \in \Q} m^*(E + q) = 0.
    \]
    Absurd.

    In particular \(E \notin \mathcal L\) as \(m^*\) is additive on \(\mathcal L\).
  \end{proof}
\end{eg}

\section{Abstract measure theory}

In this chapter we extend measure theory to arbitrary set. Most part of the theory is developed by Fréchet and Carathéodory.

\begin{definition}[\(\sigma\)-algebra]\index{\(\sigma\)-algebra}
  A \emph{\(\sigma\)-algebra} on a set \(X\) is a Boolean algebra stable under countable unions.
\end{definition}

\begin{definition}[measurable space]\index{measurable space}
  A \emph{measurable space} is a couple \((X, \mathcal A)\) where \(X\) is a set and \(\mathcal A\) is a \(\sigma\)-algebra on \(X\).
\end{definition}

\begin{definition}[measure]\index{measure}
  A \emph{measure} on \((X, \mathcal A)\) is a map \(\mu: \mathcal A \to [0, \infty]\) such that
  \begin{enumerate}
  \item \(\mu(\emptyset) = 0\),
  \item \(\mu\) is countably additive (also known as \(\sigma\)-additive), i.e.\ for every family \((A_n)_{n \geq 1}\) of disjoint subsets in \(\mathcal A\), have
    \[
      \mu (\bigcup_{n \geq 1} A_n) = \sum_{n \geq 1} \mu(A_n).
    \]
  \end{enumerate}

  The triple \((X, \mathcal A, \mu)\) is called a measure space.
\end{definition}

\begin{eg}\leavevmode
  \begin{enumerate}
  \item \((\R^d, \mathcal L, m)\) is a measure space.
  \item \((X, 2^X, \#)\) where \(\#\) is the counting measure.
  \end{enumerate}
\end{eg}

\begin{proposition}
  Let \((X, \mathcal A, \mu)\) be a measure space. Then
  \begin{enumerate}
  \item \(\mu\) is monotone: \(A \subseteq B\) implies \(\mu(A) \subseteq \mu(B)\),
  \item \(\mu\) is countably subadditive: \(\mu (\bigcup_{n \geq 1} A_n) \leq \sum_{n \geq 1} \mu(A_n)\),
  \item upward monotone convergence: if
    \[
      E_1 \subseteq E_2 \subseteq \dots \subseteq E_n \subseteq \dots
    \]
    then
    \[
      \mu (\bigcup_{n \geq 1} E_n) = \lim_{n \to \infty} \mu(E_n) = \sup_{n \geq 1} \mu(E_n).
    \]
  \item downard monotone convergence: if
    \[
      E_1 \supseteq E_2 \supseteq \dots \supseteq E_n \supseteq \dots
    \]
    and \(\mu(E_1) < \infty\) then
    \[
      \mu (\bigcap_{n \geq 1} E_n) = \lim_{n \to \infty} \mu(E_n) = \inf_{n \geq 1} \mu(E_n).
    \]
  \end{enumerate}
\end{proposition}

\begin{proof}\leavevmode
  \begin{enumerate}
  \item
    \[
      \mu(B) = \mu(A) + \underbrace{\mu(B \setminus A)}_{\geq 0}
    \]
    by additivity of \(\mu\).
  \item See example sheet. The idea is that every countable union \(\bigcup_{n \geq 1} A_n\) is a disjoint countable union \(\bigcup_{n \geq 1} B_n\) where for each \(n\), \(B_n \subseteq A_n\). It then follows by \(\sigma\)-additivity.
  \item Let \(E_0 = \emptyset\) so
    \[
      \bigcup_{n \geq 1} E_n = \bigcup_{n \geq 1} (E_n \setminus E_{n - 1}),
    \]
    a disjoint union. By \(\sigma\)-additivity,
    \[
      \mu(\bigcup_{n \geq 1} E_n) = \sum_{n \geq 1} \mu(E_n \setminus E_{n - 1})
    \]
    but for all \(N\), by additivity of \(\mu\),
    \[
      \sum_{n = 1}^N \mu(E_n \setminus E_{n - 1}) = \mu(E_N)
    \]
    so take limit. The supremum part is obvious.
  \item Apply the previous result to \(E_1 \setminus E_n\).
  \end{enumerate}
\end{proof}

\begin{remark}
  Note the \(\mu(E_1) < \infty\) condition in the last part. Counterexample: \(E_n = [n, \infty) \subseteq \R\).
\end{remark}

\begin{definition}[\(\sigma\)-algebra generated by a family]
  Let \(X\) be a set and \(\mathcal F\) be some family of subsets of \(X\). The the intersection of all \(\sigma\)-algebras on \(X\) containing \(\mathcal F\) is a \(\sigma\)-algebra, called the \(\sigma\)-algebra \emph{generated} by \(\mathcal F\) and is denoted by \(\sigma(\mathcal F)\).
\end{definition}

\begin{proof}
  Easy check. See example sheet.
\end{proof}

\begin{eg}\leavevmode
  \begin{enumerate}
  \item Suppose \(X = \coprod_{i = 1}^N X_i\), i.e.\ \(X\) admits a finite partition. Let \(\mathcal F = \{X_1, \dots, X_n\}\), then \(\sigma(\mathcal F)\) consists of all subsets that are unions of \(X_i\)'s.
  \item Suppose \(X\) is countable and let \(\mathcal F\) be the collection of all singletons. Then \(\sigma(\mathcal F) = 2^X\).
  \end{enumerate}
\end{eg}

\begin{definition}[Borel \(\sigma\)-algebra]\index{Borel \(\sigma\)-algebra}
  Let \(X\) be a topological space. The \(\sigma\)-algebra generated by open subsets of \(X\) is called the \emph{Borel \(\sigma\)-algebra} of \(X\), denoted by \(\mathcal B(X)\).
\end{definition}

\begin{proposition}
  If \(X = \R^d\) then \(\mathcal B(X) \subseteq \mathcal L\). Moreover every \(A \in \mathcal L\) can be written as a disjoint union \(A = B \cup N\) where \(B \in \mathcal B(X)\) and \(N\) is a null set.
\end{proposition}

%This gives an alternative way to define Lebesgue measure: 

\begin{proof}
  We've shown that \(\mathcal L\) is a \(\sigma\)-algebra and contains all open sets so \(\mathcal B(X) \subseteq \mathcal L\). Given \(A \in \mathcal L\), \(A^c \in \mathcal L\) so for all \(n \geq 1\) there exists \(C_n\) countable unions of (open) boxes such that \(A^c \subseteq C_n\) and \(m^*(C_n \setminus A^c) \leq \frac{1}{n}\). Take \(C = \bigcap_{n \geq 1} C_n \in \mathcal B(X)\). Thus \(B := C^c \in \mathcal B(X)\) and \(m(A \setminus B) = 0\) because \(A \setminus B = C \setminus A^c\).
\end{proof}

\begin{remark}\leavevmode
  \begin{enumerate}
  \item It can be shown that \(B(\R^d) \subsetneq \mathcal L\). In fact \(|\mathcal L| \geq 2^{\mathfrak c}\) and \(|\mathcal B(\R^d)| = \mathfrak c\).
  \item If \(\mathcal F\) is a family of subsets of a set \(X\), the Boolean algebra generated by \(\mathcal F\) can be explicitly described as
    \[
      \mathcal B(\mathcal F) = \{\text{finite unions of } F_1 \cap \dots \cap F_N: F_i \in \mathcal F \text{ or } F_i^c \in \mathcal F\}.
    \]
  \item However, this is not so for \(\sigma(\mathcal F)\). There is no ``simple'' description of \(\sigma\)-algebra generated by \(\mathcal F\). (c.f.\ Borel hierarchy in descriptive set theory and transfinite induction)
  \end{enumerate}
\end{remark}

\begin{definition}[\(\pi\)-system]\index{\(\pi\)-system}
  A family \(\mathcal F\) of subsets of a set \(X\) is called a \emph{\(\pi\)-system} if it contains \(\emptyset\) and it is closed under finite intersection.
\end{definition}

\begin{proposition}[measure uniqueness]
  Let \((X, \mathcal A)\) be a measurable space. Assume \(\mu_1\) and \(\mu_2\) are two finite measures (i.e.\ \(\mu_i(X) < \infty\)) such that \(\mu_1(F) = \mu_2(F)\) for every \(F \in \mathcal F\) where \(\mathcal F\) is a \(\pi\)-system with \(\sigma(\mathcal F) = \mathcal A\). Then \(\mu_1 = \mu_2\).
\end{proposition}

For \(\R^d\), we only have to check open boxes.

\begin{proof}
We state first the following lemma:
\begin{lemma}[Dynkin lemma]
  If \(\mathcal F\) is a \(\pi\)-system on \(X\) and \(\mathcal C\) is a family of subsets of \(X\) such that \(\mathcal F \subseteq \mathcal C\) and \(\mathcal C\) is stable under complementation and disjoint countable unions. Then \(\sigma(\mathcal F) \subseteq \mathcal C\).
\end{lemma}

Let \(\mathcal C = \{A \in \mathcal A: \mu_1(A) = \mu_2(A)\}\). Then \(\mathcal C\) is clearly stable under complementation as
\[
  \mu_i (A^c) = \mu_i(X \setminus A) = \mu_i(X) - \mu_i(A).
\]
\(\mathcal C\) is also clearly stable under countable disjoint unions by \(\sigma\)-additivity. Thus by Dynkin lemma, \(\mathcal C \supseteq \sigma(\mathcal F) = \mathcal A\).

\begin{proof}[Proof of Dynkin lemma]
  Let \(\mathcal M\) be the smallest family of subsets of \(X\) containing \(\mathcal F\) and stable under complementation and countable disjoint union (\(2^X\) is such a family and taking intersection). Sufficient to show that \(\mathcal M\) is a \(\sigma\)-algebra, as then \(\mathcal M \subseteq \mathcal C\) implies \(\sigma(\mathcal F) \subseteq \mathcal C\).

  It suffices to show \(\mathcal M\) is a Boolean algebra. Let
  \[
    \mathcal M' = \{A \in \mathcal M: A \cap B \in \mathcal M \text{ for all } B \in \mathcal F\}.
  \]
  \(\mathcal M'\) again is stable under countable disjoint unions and complementation because
  \[
    A^c \cap B = (B^c \cup (A \cap B))^c
  \]
  as a disjoint union so is in \(\mathcal M\).

  As \(\mathcal M' \supseteq \mathcal F\), by minimality of \(\mathcal M\), have \(\mathcal M = \mathcal M'\). Now let
  \[
    \mathcal M'' = \{A \in \mathcal M': A \cap B \in \mathcal M \text{ for all } B \in \mathcal M\}.
  \]
  The same argument shows that \(\mathcal M'' = \mathcal M\). Thus \(\mathcal M\) is a Boolean algebra and a \(\sigma\)-algebra.
\end{proof}
\end{proof}

\begin{proposition}[uniqueness of Lebesgue measure]
  Lebesgue measure is the unique translation invariant measure \(\mu\) on \((\R^d, \mathcal B(\R^d))\) such that
  \[
    \mu([0, 1]^d) = 1.
  \]
\end{proposition}

\begin{proof}
  Exercise. Hint: use the \(\pi\)-system \(\mathcal F\) made of all boxes in \(\R^d\) and dissecting a cube into dyadic pieces. Then approximate and use monotone.
\end{proof}

\begin{remark}\leavevmode
  \begin{enumerate}
  \item There is \emph{no} countably additive translation invariant measure on \(\R\) defined on all subsets of \(\R\). (c.f.\ Vitali's counterexample).
  \item However, the Lebesgue measure can be extended to a finitely additive measure on all subsets of \(\R\) (proof requires Hahn-Banach theorem. See IID Linear Analysis).
  \end{enumerate}
\end{remark}

Recall the construction of Lebesgue measure: we take boxes in \(\R^d\), and define elementary sets, which is the Boolean algebra generated by boxes. Then we can define Jordan measure which is finitely additive. However, this is not countably additive but analysis craves limits so we define Lebesgue measurable sets, by introducing the outer measure \(m^*\), which is built from the Jordan measure. Finally we restrict this outer measure to \(\mathcal L\). We also define the Borel \(\sigma\)-algebra, which is the same as the \(\sigma\)-algebra generated by the boxes. We show that the Borel \(\sigma\)-algebra is contained in \(\mathcal L\), and every element in \(\mathcal L\) can be written as a disjoint union of an element in the Borel \(\sigma\)-algebra and a measure zero set.

Suppose \(\mathcal B\) is a Boolean algebra on a set \(X\). Let \(\mu\) be a finitely additive measure on \(\mathcal B\). We are going to construct a measure on \(\sigma(\mathcal B)\).

\begin{theorem}[Carathéodory extension theorem]\index{Carathéodory extension theorem}
  Assume that \(\mu\) is countably additive on \(\mathcal B\), i.e.\ if \(B_n \in \mathcal B\) disjoint is such that \(\bigcup_{n \geq 1} B_n \in \mathcal B\) then \(\mu(\bigcup_{n \geq 1} B_n) = \sum_{n \geq 1} \mu(B_n)\) and assume that \(\mu\) is \(\sigma\)-finite, i.e.\ there exists \(X_m \in \mathcal B\) such that \(X = \bigcup_{m \geq 1} X_m\) and \(\mu(X_m) < \infty\), then \(\mu\) extends uniquely to a measure on \(\sigma(\mathcal B)\).
\end{theorem}

\begin{proof}
  For any \(E \subseteq X\), let
  \[
    \mu^*(E) = \inf \{\sum_{n \geq 1} \mu(B_n): E \subseteq \bigcup_{n \geq 1} B_n, B_n \in \mathcal B\}
  \]
  and call it the outer measure associated to \(\mu\). Define a subset \(E \subseteq X\) to be \(\mu^*\)-measurable if for all \(\varepsilon > 0\) there exists \(C = \bigcup_{n \geq 1} B_n\) with \(B_n \in \mathcal B\) such that \(E \subseteq C\) and
  \[
    \mu^*(C \setminus E) \leq \varepsilon.
  \]
  We denote by \(\mathcal B^*\) the set of \(\mu^*\)-measurable subsets. Claim that
  \begin{enumerate}
  \item \(\mu^*\) is countably subadditive and monotone.
  \item \(\mu^*(B) = \mu(B)\) for all \(B \in \mathcal B\).
  \item \(\mathcal B^*\) is a \(\sigma\)-algebra and contains all \(\mu^*\)-null sets and \(\mathcal B\).
  \item \(\mu^*\) is \(\sigma\)-additive on \(\mathcal B^*\).
  \end{enumerate}

  Then existence follows from the proposition as \(\mathcal B^* \supseteq \sigma(\mathcal B)\): \(\mu^*\) will be a measure on \(\mathcal B^*\) and thus on \(\sigma(\mathcal B)\). Uniqueness follows from a similar proof for Lebesgue measure via Dynkin lemma.

  \begin{proof}
    This will be very easy as we only need to adapt our previous work to the general case. Note that in a few occassion we used properties of \(\R^d\), such as openness of some sets, so be careful.
    \begin{enumerate}
    \item Same.
    \item \(\mu^*(B) \leq \mu(B)\) for all \(B \in \mathcal B\) by definition of \(\mu^*\). For the other direction, for all \(\varepsilon > 0\), there exist \(B_n \in \mathcal B\) such that \(B \subseteq \bigcup_{n \geq 1} B_n\) and \(\sum_{n \geq 1} \mu(B_n) \leq \mu^*(B) + \varepsilon\). But
      \[
        B = \bigcup_{n \geq 1} B_n \cap B = \bigcup_{n \geq 1} C_n
      \]
      where \(C_n := B_n \cap B \setminus \bigcup_{i < n} B\cap B_i\) and so \(C_n \in \mathcal B\). Thus by countable additivity
      \[
        \mu(B)
        = \sum_{n \geq 1} \mu(C_n)
        \leq \sum_{n \geq 1} \mu(B_n)
        \leq \mu^*(B) + \varepsilon
      \]
    \item \(\mu^*\)-null sets and \(\mathcal B\) are obviously in \(\mathcal B^*\). Thus it is left to show that \(\mathcal B^*\) is a \(\sigma\)-algebra. Stability under countable union is exactly the same and then we claim that \(\mathcal B^*\) is stable under complementation. This is the bit where we used closed/open sets in \(\R^d\) in the original proof. Here we use a lemma as a substitute.

      \begin{lemma}
        %\(\mathcal B^*\) is stable under countable unions and countable intersections.
        Suppose \(B_n \in \mathcal B\) then \(\bigcap_{n \geq 1} B_n \in \mathcal B^*\).
      \end{lemma}

      \begin{proof}
        First claim that if \(E = \bigcap_{n \geq 1} I_n\) where \(I_{n + 1} \subseteq I_n\) and \(I_n \in \mathcal B\) such that \(\mu(I_1) < \infty\) then \(\mu^*(E) = \lim_{n \to \infty} \mu(I_n)\) and \(E \in \mathcal B^*\): by additivity of \(\mu\) on \(\mathcal B\),
        \[
          \sum_{n = 1}^N \mu(I_n \setminus I_{n + 1})
          = \mu(I_1) - \mu(I_N)
        \]
        which converges as \(N \to \infty\) (because \(\mu(I_{n + 1}) \leq \mu(I_n)\)), so
        \[
          \sum_{n \geq N} \mu(I_n \setminus I_{n + 1}) \to 0
        \]
        as \(N \to \infty\). But LHS is greater than \(\mu^*(I_N \setminus E)\) because \(I_N \setminus E = \bigcup_{n \geq N} I_n \setminus I_{n + 1}\). Therefore \(E \in \mathcal B^*\) and
          \[
            \mu(I_n) \leq \underbrace{\mu^*(I_n \setminus E)}_{\to 0} + \underbrace{\mu^*(E)}_{\leq \mu(I_n)}
          \]
          so
          \[
            \lim_{n \to \infty} \mu(I_n) = \mu^*(E).
          \]

          Now for the actual lemma, let \(E = \bigcap_{n \geq 1} I_n\) where \(I_n \in \mathcal B\). wlog we may assume \(I_{n + 1} \subseteq I_n\). By \(\sigma\)-finiteness assumption, \(X = \bigcup_{m \geq 1} X_m\) where \(X_m \in \mathcal B\) with \(\mu(X_m) < \infty\) so
          \[
            E = \bigcup_{m \geq 1} E \cap X_m.
          \]
          By the claim for all \(m\), \(E \cap X_m \in \mathcal B^*\) so \(E \in \mathcal B^*\).
      \end{proof}
      From the lemma we can derive that \(\mathcal B^*\) is also stable under complementation: given \(E \in \mathcal B^*\), for all \(n\) there exist \(C_n = \bigcup_{i \geq 1} B_{n, i}\) where \(B_{n, i} \in \mathcal B\) such that \(E \subseteq C_n\) and \(\mu^*(C_n \setminus E) \leq \frac{1}{n}\). Now
      \[
        E^c = (\bigcup_{n \geq 1} C_n^c) \cup (E^c \setminus \bigcup_{n \geq 1} C_n^c)
      \]
      but \(C_n^c\) is a countable intersection \(\bigcap_{i \geq 1} B_{n, i}^c\) and \(E^c \setminus \bigcup_{n \geq 1} C_n^c\) is \(\mu^*\)-null so by the lemma, \(C_n^c \in \mathcal B^*\). Therefore their union is also in \(\mathcal B^*\). Since we've shown that null sets are in \(\mathcal B^*\), \(E^c \in \mathcal B^*\).
    \item We want to show \(\mu^*\) is countably additive on \(\mathcal B^*\). Recall that \(\mu\) is \(\sigma\)-finite: there exists \(X_m \in \mathcal B\) such that \(X = \bigcup_{m \geq 1} X_m\), \(\mu(X_m) < \infty\). We say \(E \subseteq X\) is \emph{bounded} if there exists \(m\) such that \(E \subseteq X_m\). It is then enough to show countable additivity for bounded sets by the same argument as before: write \(X = \bigcup_{m \geq 1} \tilde X_m\) where \(\tilde X_m = X_m \setminus \bigcup_{i < m} X_i \in \mathcal B\) so this is a disjoint union. Then if \(E = \bigcup_{n \geq 1} E_n\) as a disjoint union then
      \[
        E = \bigcup_{n \geq 1} \bigcup_{m \geq 1} (E_n \cap \tilde X_m)
      \]
      which is also a countable disjoint union.

      Given \(E\), if we can show finite additivity then
      \[
        \sum_{n = 1}^n \mu^*(E_n)
        = \mu^*(\bigcup_{n = 1}^N E_n)
        \leq \mu^*(E)
        \leq \sum_{n \geq 1} \mu^*(E_n)
      \]
      take limit as \(N \to \infty\) to have equality throughout.
      
      It suffices to prove finite additivity when \(E\) and \(F\) are countable intersections of sets from \(\mathcal B\): \(E,F \in \mathcal B^*\) so for \(\varepsilon > 0\) there exists \(C, D\) countable intersections of sets from \(\mathcal B\) such that \(C \subseteq E, D \subseteq F\) and
      \begin{align*}
        \mu^*(E) &\leq \mu^*(C) + \varepsilon \\
        \mu^*(F) &\leq \mu^*(D) + \varepsilon
      \end{align*}
      As \(E \cap F = \emptyset\) and \(C \subseteq E, D \subseteq F\), \(C \cap D = \emptyset\) so by finite additivity,
      \[
        \mu^*(E) +\mu^*(F) \leq 2\varepsilon + \mu^*(C \cup D) \leq 2 \varepsilon + \mu^*(E \cup F).
      \]
      As usual, reverse holds by subadditivity.

      Finally for \(E = \bigcap_{n \geq 1} I_n, F = \bigcap_{n \geq 1} J_n\) bounded, wlog assume \(I_{n + 1} \subseteq I_n, J_{n + 1} \subseteq J_n\). \(\mu(I_n), \mu(J_n) < \infty\). Now use claim 3,
      \begin{align*}
        \mu^*(E) &= \lim_{n \to \infty} \mu^*(I_n) \\
        \mu^*(F) &= \lim_{n \to \infty} \mu^*(J_n)
      \end{align*}
      so
      \[
        \mu^*(E) + \mu^*(F)
        = \lim_{n \to \infty} \mu(I_n) + \mu(J_n)
        = \lim_{n \to \infty} (\mu(I_n \cup J_n) + \mu(I_n \cap J_n))
      \]
      But
      \begin{align*}
        \bigcap_{n \geq 1} (I_n \cap J_n) &= E \cap F = \emptyset \\
        \bigcap_{n \geq 1} (I_n \cup J_n) &= E \cup F
      \end{align*}
      so by claim 3
      \begin{align*}
        \lim_{n \to \infty} \mu(I_n \cap J_n) &= 0 \\
        \lim_{n \to \infty} \mu(I_n \cup J_n) &= \mu^*(E \cup F)
      \end{align*}
      which finishes the proof.
    \end{enumerate}
  \end{proof}
\end{proof}

\begin{remark}
  We prove that every set in \(\mathcal B^*\) is a disjoint union \(E = F \cup N\) where \(F \in \sigma(\mathcal B)\) and \(N\) is \(\mu^*\)-null.
\end{remark}

\begin{definition}[completion]\index{completion}
  We say that \(\mathcal B^*\) is the \emph{completion} of \(\sigma(\mathcal B)\) with respect to \(\mu\).
\end{definition}

\begin{eg}
  \(\mathcal L\) is the completion of \(\mathcal B(\R^d)\) in \(\R^d\).
\end{eg}

\section{Integration and measurable functions}

\begin{definition}[measurable function]\index{measurable}
  Let \((X, \mathcal A)\) be a measurable space. A function \(X \to \R\) is called \emph{measurable} or \emph{\(\mathcal A\)-measurable} if for all \(t \in \R\),
  \[
    \{x \in X: f(x) < t\} \in \mathcal A.
  \]
\end{definition}

\begin{remark}
  The \(\sigma\)-algebra generated by intervals \((-\infty, t)\) where \(t \in \R\) is the Borel \(\sigma\)-algebra of \(\R\), denote \(\mathcal B(\R)\). Thus for every measurable function \(f: X \to \R\), the preimage \(f^{-1}(B) \in \mathcal A\) for all \(B \in \mathcal B(\R)\). However, it is \emph{not} true that \(f^{-1}(L) \in \mathcal A\) for any \(L \in \mathcal L\).
\end{remark}

\begin{remark}
  If \(f\) is allowed to take the values \(+\infty\) and \(-\infty\) we will say that \(f\) is measurable if additionally \(f^{-1}(\{+\infty\}) \in \mathcal A\) and \(f^{-1}(\{-\infty\}) \in \mathcal A\).
\end{remark}

More generally,

\begin{definition}[measurable map]\index{measurable}
  Suppose \((X, \mathcal A)\) and \((Y, \mathcal B)\) are measurable spaces. A map \(f: X \to Y\) is \emph{measurable} if for all \(B \in \mathcal B\), \(f^{-1}(B) \in \mathcal A\).
\end{definition}

\begin{proposition}\leavevmode
  \begin{enumerate}
  \item The composition of measurable maps is measurable.
  \item If \(f, g: (X, \mathcal A) \to \R\) are measurable functions then \(f + g, fg\) and \(\lambda f\) for \(f \in \R\) are also measurable.
  \item If \((f_n)_{n \geq 1}\) is a sequence of measurable functions on \((X, \mathcal A)\) then so are \(\sup_n f_n, \inf_n f_n, \limsup_n f_n\) and \(\liminf_n f_n\).
  \end{enumerate}
\end{proposition}

\begin{proof}\leavevmode
  \begin{enumerate}
  \item Obvious.
  \item Follows from 1 once it's shown that \(+: \R^2 \to \R\) and \(\times: \R^2 \to \R\) are measurable (w.r.t.\ Borel sets). The sets
    \begin{align*}
      &\{(x, y): x + y < t\} \\
      &\{(x, y): xy < t\}
    \end{align*}
    are open in \(\R^2\) and hence Borel.
  \item \(\inf_n f_n(x) < t\) if and only if
    \[
      x \in \bigcup_n \{x: f_n(x) < t\}
    \]
    and similar for \(\sup\). Similarly \(\limsup_n f_n(x) < t\) if and only if
    \[
      x \in \bigcup_{m \geq 1} \bigcap_{k \geq 1} \bigcup_{n \geq k} \{x: f_n(x) < t - \frac{1}{m}\}.
    \]
  \end{enumerate}
\end{proof}

\begin{proposition}
  \(f = (f_1, \dots, f_d): (X, \mathcal A) \to (\R^d, \mathcal B(\R^d))\) where \(d \geq 1\) is measurable if and only if each \(f_i: X \to \R\) is measurable.
\end{proposition}

\begin{proof}
  One direction is easy: suppose \(f\) is measurable then
  \[
    \{x: f_i(x) < t\} = f^{-1}(\{y \in \R^d: y_i < t\}),
  \]
  which is open so \(f_i\) is measurable.

  Conversely, suppose \(f_i\) is measurable. Then
  \[
    f^{-1}( \prod_{i = 1}^d [a_i, b_i] )
    = \bigcap_{i = 1}^d \{x: a_i \leq f_i(x) \leq b_i\}
  \]
  As the boxes generate the Borel sets, done.
\end{proof}

\begin{eg}\leavevmode
  \begin{enumerate}
  \item Let \((X, \mathcal A)\) be a measurable space and \(E \subseteq X\). Then \(E \in \mathcal A\) if and only if \(\mathbf 1_E\), the indicator function on \(E\), is \(\mathcal A\)-measurable.
  \item If \(X = \coprod_{i = 1}^N X_i\) and \(\mathcal A\) is the Boolean algebra generated by the \(X_i\)'s. A function \(f: (X, \mathcal A) \to \R\) is measurable if and only if \(f\) is constant on each \(X_i\). In this case the vector space of measurable functions has dimension \(N\).
  \item Every continuous function \(f: \R^d \to \R\) is measurable.
  \end{enumerate}
\end{eg}

\begin{definition}[Borel measurable]\index{Borel measurable}
  If \(X\) is a topological space, \(f: X \to \R\) is \emph{Borel} or \emph{Borel measurable} if it is \(\mathcal B(X)\)-measurable.
\end{definition}

\begin{definition}[simple function]\index{simple function}
  A function \(f\) on \((X, \mathcal A)\) is called \emph{simple} if
  \[
    f = \sum_{i = 1}^n a_i \mathbf 1_{A_i}
  \]
  for some \(a_i \geq 0\) and \(A_i \in \mathcal A\).
\end{definition}

Of course simple functions are measurable.

\begin{lemma}
  If a simple function can be written in two ways
  \[
    f = \sum_{i = 1}^n a_i \mathbf 1_{A_i} = \sum_{j = 1}^s b_j \mathbf 1_{B_j}
  \]
  then
  \[
    \sum_{i = 1}^n a_i \mu(A_i) = \sum_{j = 1}^s b_j \mu(B_j)
  \]
  for any measure \(\mu\) on \((X, \mathcal A)\).
\end{lemma}

\begin{proof}
  Example sheet 1.
\end{proof}

\begin{definition}[integral of a simple function with respect to a measure]\index{integral with respect to a measure}
  The \emph{\(\mu\)-integral} of \(f\) is defined by
  \[
    \mu(f) := \sum_{i = 1}^n a_i \mu(A_i).
  \]
\end{definition}

\begin{remark}\leavevmode
  \begin{enumerate}
  \item The lemma says that the integral is well-defined.
  \item We also use the notation \(\int_X f d\mu\) to denote \(\mu(f)\).
  \end{enumerate}
\end{remark}

\begin{proposition}
  \(\mu\)-integral satisfies, for all simple functions \(f\) and \(g\),
\begin{enumerate}
\item linearity: for all \(\alpha, \beta \geq 0\), \(\mu(\alpha f + \beta g) = \alpha \mu(f) + \beta \mu(g)\).
\item positivity: if \(g \leq f\) then \(\mu(g) \leq \mu(f)\).
\item if \(\mu(f) = 0\) then \(f = 0\) \(\mu\)-almost everywhere, i.e.\ \(\{x \in X: f(x) \neq 0\}\) is a \(\mu\)-null set.
\end{enumerate}
\end{proposition}

\begin{proof}
  Obvious from definition and lemma.
\end{proof}

\begin{definition}
  If \(f \geq 0\) and measurable on \((X, \mathcal A)\), define
  \[
    \mu(f) = \sup \{\mu(g): g \text{ simple }, g \leq f\} \in [0, +\infty].
  \]
\end{definition}

\begin{remark}
  This is consistent with the definition for \(f\) simple, due to positivity.
\end{remark}

\begin{definition}[integrable]\index{integrable}
  If \(f\) is an arbitrary measurable function on \((X, \mathcal A)\) we say \(f\) is \emph{\(\mu\)-integrable} if
  \[
    \mu(|f|) < \infty.
  \]
\end{definition}

\begin{definition}[integral with respect to a measure]\index{integral with respect to a measure}
  If \(f\) is \(\mu\)-integrable, then we define its \emph{\(\mu\)-integral} by
  \[
    \mu(f) = \mu(f^+) - \mu(f^-)
  \]
  where \(f^+ = \max\{0, f\}\) and \(f^- = (-f)^+\).
\end{definition}

\begin{note}
  \begin{align*}
    |f| &= f^+ + f^- \\
    f &= f^+ - f^-
  \end{align*}
\end{note}

\begin{theorem}[monotone convergence theorem]\index{monotone convergence theorem}
  \label{thm:monotone convergence theorem}
  Let \((f_n)_{n \geq 1}\) be a sequence of measurable functions on a measure space \((X, \mathcal A, \mu)\) such that
  \[
    0 \leq f_1 \leq f_2 \leq \dots \leq f_n \leq \dots
  \]
  Let \(f = \lim_{n \to \infty} f_n\). Then
  \[
    \mu(f) = \lim_{n \to \infty} \mu(f_n).
  \]
\end{theorem}

\begin{lemma}
  If \(g\) is a simple function on \((X, \mathcal A, \mu)\), the map
  \begin{align*}
    m_g: \mathcal A &\to [0, \infty] \\
    E &\mapsto \mu(\mathbf 1_E g)
  \end{align*}
  is a measure on \((X, \mathcal A)\).
\end{lemma}

\begin{proof}
  Write \(g = \sum_{i = 1}^r a_i \mathbf 1_{A_i}\) so \(g \mathbf 1_E = \sum_{i = 1}^r a_i \mathbf 1_{A_i \cap E}\) so
  \[
    \mu(\mathbf 1_E g) = \sum_{i = 1}^r a_i \mu(A_i \cap E).
  \]
  By a question on example sheet this is well-defined. Then \(\sigma\)-additivity follows immediately from \(\sigma\)-additivity of \(\mu\).
\end{proof}

\begin{proof}[Proof of \nameref{thm:monotone convergence theorem}]
  \(f_n \leq f_{n + 1} \leq f\) by assumption so
  \[
    \mu(f_n) \leq \mu(f_{n + 1}) \leq \mu(f)
  \]
  by definition of integral so
  \[
    \lim_{n \to \infty} \mu(f_n) \leq \mu(f),
  \]
  although RHS may be infinite.

  Let \(g\) be any simple function with \(g \leq f\). Need to show that \(\mu(g) \leq \lim_{n \to \infty} \mu(f_n)\). Pick \(\varepsilon > 0\). Let
  \[
    E_n = \{x \in X: f_n(x) \geq (1 - \varepsilon) g(x) \}.
  \]
  Then \(X = \bigcup_{n \geq 1} E_n\) and \(E_n \subseteq E_{n + 1}\). So we may apply upward monotone convergence for sets to measure \(m_g\) and get
  \[
    \lim_{n \to \infty} m_g(E_n) = m_g(X) = \mu(g \mathbf 1_X) = \mu(g).
  \]
  But
  \[
    (1 - \varepsilon) m_g(E_n) = \mu((1 - \varepsilon) g \mathbf 1_{E_n})) \leq \mu(f_n)
  \]
  because \((1 - \varepsilon) g \mathbf 1_{E_n}\) is a simple function smaller than \(f_n\). Taking limit,
  \[
    (1 - \varepsilon) \mu(g) \leq \lim_{n \to \infty} \mu(f_n)
  \]
  which holds for all \(\varepsilon\). So
  \[
    \mu(g) \leq \lim_{n \to \infty} \mu(f_n).
  \]
\end{proof}

\begin{lemma}
  \label{lem:approximation by simple functions}
  If \(f \geq 0\) is a measurable function on \((X, \mathcal A)\) then there is a sequence of simple functions \((g_n)_{n \geq 1}\)
  \[
    0 \leq g_n \leq g_{n + 1} \leq f
  \]
  such that for all \(x \in X\), \(g_n(x) \uparrow f(x)\).
\end{lemma}

\begin{notation}
  \(g_n \uparrow f\) means that \(\lim_{n \to \infty} g_n(x) = f(x)\) and \(g_{n + 1} \geq g_n\).
\end{notation}

\begin{proof}
  We can take
  \[
    g_n = \frac{1}{2^n} \floor*{2^n \min \{f, n\}}
  \]
  pointwise. Check that \(\floor{2y} \geq 2 \floor{y}\) for all \(y \geq 0\).
\end{proof}

\begin{proposition}
  Basic properties of the integral (for positive functions): suppose \(f, g\) are measurable on \((X, \mathcal A, \mu)\).
  \begin{enumerate}
  \item linearity: for all \(\alpha, \beta \geq 0\), \(\mu(\alpha f + \beta y) = \alpha \mu(f) + \beta \mu(g)\).
  \item positivity: if \(0 \leq f \leq g\) then \(\mu(f) \leq \mu(g)\).
  \item if \(\mu(f) = 0\) then \(f = 0\) \(\mu\)-almost everywhere.
  \item if \(f = g\) \(\mu\)-almost everywhere then \(\mu(f) = \mu(g)\).
  \end{enumerate}
\end{proposition}

\begin{proof}\leavevmode
  \begin{enumerate}
  \item Follows from the same property for simple functions and from \Cref{lem:approximation by simple functions} combined with monotone convergence theorem.
  \item Obvious from definition.
  \item
    \[
      \{x \in X: f(x) \neq 0\} = \bigcup_{n \geq 0} \{x \in X: f(x) > \frac{1}{n}\}
    \]
    set \(g_n = \frac{1}{n} \mathbf 1_{\{x \in X: f(x) > 1/n\}}\) which is simple and \(g_n \leq f\) so by definition of integral \(\mu(g_n) \leq \mu(f)\) so \(\mu(g_n) = 0\), i.e.\ \(\mu(\{x: f(x) > \frac{1}{n}) = 0\).
  \item Note that if \(E \in \mathcal A\), \(\mu(E^c) = 0\) then
    \[
      \mu(h \mathbf 1_E) = \mu(h)
    \]
    for all \(h\) simple. Thus it holds for all \(h \geq 0\) measurable. Now take \(E = \{x: f(x) = g(x)\}\).
  \end{enumerate}
\end{proof}

\begin{proposition}[linearity of integral]
  Suppose \(f, g\) are \(\mu\)-integrable functions and \(\alpha, \beta \in \R\). Then \(\alpha f + \beta g\) is \(\mu\)-integrable and
  \[
    \mu(\alpha f + \beta g) = \alpha \mu(f) + \beta \mu(g).
  \]
\end{proposition}

\begin{proof}
  We have shown the case when \(\alpha, \beta \geq 0\) and \(f, g \geq 0\). In the general case, use the positive and negative parts.
\end{proof}

\begin{lemma}[Fatou's lemma]\index{Fatou's lemma}
  Suppose \((f_n)_{n \geq 1}\) is a sequence of measurable functions on \((X, \mathcal A, \mu)\) such that \(f_n \geq 0\) for all \(n\). Then
  \[
    \mu(\liminf_{n \to \infty} f_n) \leq \liminf_{n \to \infty} \mu(f_n).
  \]
\end{lemma}

\begin{remark}
  We may not have equality: let \(f_n = \mathbf 1_{[n, n + 1]}\) on \((\R, \mathcal L, m)\). Then \(\mu(f_n) = 1\) but \(\lim_{n \to \infty} f_n = 0\).
\end{remark}

\begin{proof}
  Let \(g_n := \inf_{k \geq n} f_k\). Then \(g_{n + 1} \geq g_n \geq 0\) so by monotone convergence theorem, \(\mu(g_n) \uparrow \mu(g)\) as \(n \to \infty\) where \(g = \lim_{n \to \infty} g = \liminf_{n \to \infty} f_n\) and \(g_n \leq f_n\) so \(\mu(g_n) \leq \mu(f_n)\) for all \(n\). Take \(n \to \infty\),
  \[
    \mu(g) \leq \liminf_{n \to \infty} \mu(f_n).
  \]
\end{proof}

In both monotone convergence theorem and Fatou's lemma we assumed that the sequence of functions is nonnegative. There is another version of convergence theorem where we replace nonnegativity by domination:

\begin{theorem}[Lebesgue's dominated convergence theorem]\index{Lebesgue's dominated convergence theorem}
  Let \((f_n)_{n \geq 1}\) be a sequence of measurable functions on \((X, \mathcal A, \mu)\) and \(g\) a \(\mu\)-integrable function on \(X\). Assume \(|f_n| \leq g\) for all \(n\) (domination assumption) and assume for all \(x \in X\), \(\lim_{n \to \infty} f_n(x) = f(x)\). Then \(f\) is \(\mu\)-integrable and
  \[
    \mu(f) = \lim_{n \to \infty} \mu(f_n).
  \]
\end{theorem}

This allows us to swap limit and integral.

\begin{proof}
  \(|f_n| \leq g\) so \(|f| \leq g\) so \(\mu(|f|) \leq \mu(g) < \infty\) and \(f\) is integrable. Note that \(g + f_n \geq 0\) so by Fatou's lemma,
  \[
    \mu(\liminf_{n \to \infty} (g + f_n)) \leq \liminf_{n \to \infty} \mu(g + f_n).
  \]
  But \(\liminf_{n \to \infty} (g + f_n) = g + f\) and by linearity \(\mu(g + f_n) = \mu(g) + \mu(f_n)\), so
  \[
    \mu(g) + \mu(f) \leq \mu(g) + \liminf_{n \to \infty} \mu(f_n),
  \]
  i.e.
  \[
    \mu(f) \leq \liminf_{n \to \infty} \mu(f_n).
  \]
  Do the same with \(g - f_n\) in place of \(g + f_n\), get
  \[
    \mu(-f) \leq \liminf_{n \to \infty} \mu(-f_n) = - \limsup_{n \to \infty} \mu(f_n)
  \]
  so
  \[
    \mu(f) = \lim_{n \to \infty} \mu(f_n).
  \]
\end{proof}

\begin{corollary}[exchanging integral and summation]
  Let \((X, \mathcal A, \mu)\) be a measure space and let \((f_n)_{n \geq 1}\) be a sequence of measurable functions on \(X\).
  \begin{enumerate}
  \item If \(f_n \geq 0\) then
    \[
      \mu(\sum_{n \geq 1} f_n) = \sum_{n \geq 1} \mu(f_n).
    \]
  \item If \(\sum_{n \geq 1} |f_n|\) is \(\mu\)-integrable then \(\sum_{n \geq 1} f_n\) is \(\mu\)-integrable and
    \[
      \mu(\sum_{n \geq 1} f_n) = \sum_{n \geq 1} \mu(f_n).
    \]
  \end{enumerate}
\end{corollary}

\begin{proof}\leavevmode
  \begin{enumerate}
  \item Let \(g_N = \sum_{n = 1}^N f_n\), then \(g_N \uparrow \sum_{n \geq 1} f_n\) as \(N \to \infty\) so the result follows from monotone convergence theorem.
  \item Let \(g = \sum_{n \geq 1} |f_n|\) and \(g_N\) as above. Then \(|g_N| \leq g\) for all \(N\) so the domination assumption holds. The result thus follows from dominated convergence theorem.
  \end{enumerate}
\end{proof}

\begin{corollary}[differentiation under integral sign]
  Let \((X, \mathcal A, \mu)\) be a measure space. Let \(U \subseteq \R\) be an open set and let \(f: U \times X \to \R\) be such that
  \begin{enumerate}
  \item \(x \mapsto f(t, x)\) is \(\mu\)-integrable for all \(t \in U\),
  \item \(t \mapsto f(t, x)\) is differentiable for all \(x \in X\),
  \item domination: there exists \(g: X \to \R\) \(\mu\)-integrable such that for all \(t \in U, x \in X\),
    \[
      \frac{\partial f}{\partial t} (t, x) \leq g(x).
    \]
  \end{enumerate}
  Then \(x \mapsto \frac{\partial f}{\partial t}(t, x)\) is \(\mu\)-integrable for all \(t \in U\) and if we set \(F(t) = \int_X f(t, x) d\mu\) then \(F\) is differentiable and 
  \[
    F'(t) = \int_X \frac{\partial f}{\partial t}(t, x) d\mu.
  \]
\end{corollary}

\begin{proof}
  Pick \(h_n > 0, h_n \to 0\) and study
  \[
    g_n(t, x) := \frac{1}{h_n} (f(t + h_n, x) - f(t, x)).
  \]
  Then
  \[
    \lim_{n \to \infty} g_n(t, x) = \frac{\partial f}{\partial t}(t, x).
  \]
  By mean value theorem, there exists \(\theta_{t, n, x} \in [t, t + h_n]\) such that
  \[
    g_n(t, x) = \frac{\partial f}{\partial t}(\theta, x)
  \]
  so
  \[
    |g_n(t, x)| \leq g(x)
  \]
  by domination assumption. Now apply dominated convergence theorem.
\end{proof}

\begin{remark}\leavevmode
  \begin{enumerate}
  \item If \(f: [a, b] \to \R\) is continuous where \(a < b\) in \(\R\), then \(f\) is \(m\)-integrable (where \(m\) is the Lebesgue measure) and \(m(f) = \int_a^b f(x)dx\) is the Riemann integral. In general if \(f\) is only assumed to be bounded, then \(f\) will be Riemann integrable if and only if the points of discontinuity of \(f\) is an \(m\)-null set. See example sheet 2.
  \item If \(g \in \GL_d(\R)\) and \(f \geq 0\) is Borel measurable on \(\R^d\), then
    \[
      m(f \compose g) = \frac{1}{|\det g|} m(f).
    \]
    See example sheet 2. In particular \(m\) is invariant under linear transformation with unit determinant, e.g.\ rotation.
  \end{enumerate}
\end{remark}

\begin{remark}
  In each of monotone convergence theorem, Fatou's lemma and dominated convergence theorem, we can replace pointwise assumption by the corresponding \(\mu\)-almost everywhere. The same conclusion holds. Indeed, let
  \[
    E = \{x \in X: \text{ assumptions hold at } x\}
  \]
  so \(E^c\) is a \(\mu\)-null set. Replace each \(f_n\) and \(g\) by \(\mathbf 1_E f_n\). Then assumptions will hols everywhere as \(\mu(f \mathbf 1_E) = \mu(f)\) for all \(f\) (measurable?)
\end{remark}

\section{Product measures}

\begin{definition}[product \(\sigma\)-algebra]\index{product \(\sigma\)-algebra}
  Let \((X, \mathcal A)\) and \((Y, \mathcal B)\) be measurable spaces. The \(\sigma\)-algebra of subsets of \(X \times Y\) generated by the product sets \(E \times F\) where \(E \in \mathcal A, F \in \mathcal B\) is called the \emph{product \(\sigma\)-algebra} of \(\mathcal A\) and \(\mathcal B\) and is denoted by \(\mathcal A \otimes B\).
\end{definition}

\begin{remark}\leavevmode
  \begin{enumerate}
  \item By analogy with the notion of product topology, \(\mathcal A \otimes \mathcal B\) is the smallest \(\sigma\)-algebra of subsets of \(X \times Y\) making the two projection maps measurable.
  \item \(\mathcal B(\R^{d_1}) \otimes \mathcal B(\R^{d_2}) = \mathcal B(\R^{d_1 + d_2})\). See example sheet. However this is not so for \(\mathcal L(\R^d)\).
  \end{enumerate}
\end{remark}

\begin{lemma}
  If \(E \subseteq X \times Y\) is \(\mathcal A \otimes \mathcal B\)-measurable then for all \(x \in X\), the slice
  \[
    E_x = \{y \in Y: (x, y) \in E\}
  \]
  is in \(\mathcal B\).
\end{lemma}

\begin{proof}
  Let
  \[
    \mathcal E = \{E \subseteq X \times Y: E_x \in \mathcal B \text{ for all } x \in X\}.
  \]
  Note that \(\mathcal E\) contains all product sets \(A \times B\) where \(A \in \mathcal A, B \in \mathcal B\). \(\mathcal E\) is a \(\sigma\)-algebra: if \(E \in \mathcal E\) then \(E^c \in \mathcal E\) and if \(E_n \in \mathcal E\) then \(\bigcup E_n \in \mathcal E\) since \((E^c)_x = (E_x)^c\).
\end{proof}

\begin{lemma}
  Assume \((X, \mathcal A, \mu)\) and \((Y, \mathcal B, \nu)\) are \(\sigma\)-finite measure spaces. Let \(f: X \times Y \to [0, +\infty]\) be \(\mathcal A \otimes \mathcal B\)-measurable. Then
  \begin{enumerate}
  \item for all \(x \in X\), the function \(y \mapsto f(x, y)\) is \(\mathcal B\)-measurable.
  \item for all \(x \in X\), the map \(x \mapsto \int_Y f(x, y) d\nu(y)\) is \(\mathcal A\)-measurable.
  \end{enumerate}
\end{lemma}

\begin{proof}\leavevmode
  \begin{enumerate}
  \item In case \(f = \mathbf 1_E\) for \(E \in \mathcal A \otimes \mathcal B\) the function \(y \mapsto f(x, y)\) is just \(y \mapsto \mathbf 1_{E_x}(y)\), which is measurable by the previous lemma.

    In general, it is true for \(f\) simple functions and thus for all measurable functions by taking pointwise limit.
  \item By the same reduction we may assume \(f = \mathbf 1_E\) for some \(E \in \mathcal A \otimes \mathcal B\). Now let \(Y = \bigcup_{m \geq 1} Y_m\) with \(\nu(Y_m) < \infty\). Let
    \[
      \mathcal E = \{E \in \mathcal A \otimes \mathcal B: x \mapsto \nu(E_x \cap Y_m) \text{ is \(\mathcal A\)-measurable for all } m\}.
    \]
    \(\mathcal E\) contains all product sets \(E = A \times B\) where \(A \in \mathcal A, B \in \mathcal B\) because \(\nu(E_x \cap Y_m) = \mathbf 1_{x \in \mathcal A} \nu(B \cap Y_m)\). \(\mathcal E\) is stable under complementation:
    \[
      \nu((E^c)_x \cap Y_m) = \nu(Y_m) - \nu(Y_m \cap E_x)
    \]
    where LHS is \(\nu\)-measurable. \(\mathcal E\) is stable under disjoint countable union: let \(E = \bigcup_{n \geq 1} E_n\) where \(E_n \in \mathcal E\) disjoint. Then by \(\sigma\)-additivity
    \[
      \nu(E_x \cap Y_m) = \sum_{n \geq 1} \nu((E_n)_x \cap Y_m)
    \]
    which is \(\mathcal A\)-measurable.

    The product sets form a \(\pi\)-system and generates the product measure so by Dynkin lemma \(\mathcal E = \mathcal A \otimes \mathcal B\).
  \end{enumerate}
\end{proof}

\begin{definition}[product measure]\index{product measure}
  Let \((X, \mathcal A, \mu)\) and \((Y, \mathcal B, \nu)\) be measure spaces and \(\mu, \nu\) \(\sigma\)-finite. Then there exists a unique \emph{product measure}, denoted by \(\mu \otimes \nu\), on \(\mathcal A \otimes \mathcal B\) such that for all \(A \in \mathcal A, B \in \mathcal B\),
  \[
    \mu \otimes \nu (A \times B) = \mu(A)\nu(B).
  \]
\end{definition}

\begin{proof}
  Uniqueness follows from Dynkin lemma. For existence, set
  \[
    \sigma(E) = \int_X \nu(E_x) d\mu(x).
  \]
  \(\sigma\) is well-defined because \(x \mapsto \nu(E_x)\) is \(\mathcal A\)-measurable by lemma 2. \(\sigma\) is countably-additive: suppose \(E = \bigcup_{n \geq 1} E_n\) where \(E_n \in \mathcal A \otimes \mathcal B\) disjoint, then
  \[
    \sigma(E)
    = \int_X \nu(E_x) d\mu(x)
    = \int_X \sum_{n \geq 1} \nu((E_n)_x) d\mu x
    = \sum_{n \geq 1} \int_X \nu((E_n)_x) d\mu(x)
    = \sum_{n \geq 2} \sigma(E_n)
  \]
  by a corollary of MCT.
\end{proof}

\begin{theorem}[Tonelli-Fubini]\index{Tonelli-Fubini theorem}
  Let \((X, \mathcal A, \mu)\) and \((Y, \mathcal B, \nu)\) be \(\sigma\)-finite measure spaces.
  \begin{enumerate}
  \item Let \(f: X \times Y \to [0, +\infty]\) be \(\mathcal A \otimes \mathcal B\)-measurable. Then
    \[
      \int_{X \times Y} f(x, y) d(\mu \otimes \nu)
      = \int_X \int_Y f(x, y) d\nu(y) d\mu(x)
      = \int_X \int_X f(x, y) d\mu(x) d\nu(y).
    \]
  \item If \(f: X \times Y \to \R\) is \(\mu \otimes \nu\)-integrable then for \(\mu\)-almost everywhere \(x\), \(y \mapsto f(x, y)\) is \(\nu\)-integrable, and for \(\nu\)-almost everywhere \(y\), \(x \mapsto f(x, y)\) is \(\mu\)-integrable and
    \[
      \int_{X \times Y} f(x, y) d(\mu \otimes \nu)
      = \int_X \int_Y f(x, y) d\nu(y) d\mu(x)
      = \int_X \int_X f(x, y) d\mu(x) d\nu(y).
    \] 
  \end{enumerate}
\end{theorem}

Without the nonnegativity or integrability assumption, the result is false in general. For example for \(X = Y = \N\), let \(\mathcal A = \mathcal B\) be discrete \(\sigma\)-algebras and \(\mu = \nu\) counting measure. Let \(f(n, m) = \mathbf 1_{n = m} - \mathbf 1_{n = m + 1}\). Check that
\begin{align*}
  \sum_{n \geq 1} f(n, m) &= 0 \\
  \sum_{m \geq 1} f(n, m) &=
                            \begin{cases}
                              0 & n \geq 2 \\
                              1 & n = 1
                            \end{cases}
\end{align*}
so
\[
  \sum_{n \geq 1} \sum_{m \geq 1} f(n, m) \neq \sum_{m \geq 1} \sum_{n \geq 1} f(n, m).
\]

\begin{proof}\leavevmode
  \begin{enumerate}
  \item The result holds for \(f = \mathbf 1_E\) where \(E \in \mathcal A \otimes \mathcal B\) by the definition of product measure and lemma 2, so it holds for all simple functions. Now take limits and apply MCT.
  \item Write \(f = f^+ - f^-\) and apply 1.
  \end{enumerate}
\end{proof}

\begin{note}\leavevmode
  \begin{enumerate}
  \item The Lebesgue measure \(m_d\) on \(\R^d\) is equal to \(m_1 \otimes \dots \otimes m_1\), because it is true on boxes and extend by uniqueness of measure.
  \item \(E \in \mathcal A \otimes \mathcal B\) if is \(\mu \otimes \nu\)-null if and only if for \(\mu\)-almost every \(x\), \(\nu(E_x) = 0\).
  \end{enumerate}
\end{note}

\section{Foundations of probability theory}

Modern probability theory was founded by Kolmogorov, who formualted an axiomatic foundation of probability thoery in 1933 in the thesis \emph{Foundations on the Theory of Probability}. He defined a probability space to be a measure space \((\Omega, \mathcal F, \P)\). \(\Omega\) is the universe of possible \emph{outcomes}. However, we wouldn't be able to assign probability to every single outcome unless the space is discrete. Instead we are interest in studying some subsets of \(\Omega\), which are called \emph{events} and contained in \(\mathcal F\). Finally \(\P\) is a probability measure with \(\P(\Omega) = 1\). Thus for \(A \in \mathcal F\), \(\P(A \text{ occurs}) \in [0, 1]\).

The axioms of a probability space are interpreted as follow.
% \begin{enumerate}
% \item If \(A\) and \(B\) never occurs simultaneously then \(\P(A \text{ or } B = \P(A) + \P(B)\).
% \item \(\sigma\)-additivity is slightly more difficult to justify. It is equivalent to \emph{continuity}: if \(A_{n + 1} \supseteq A_n\) and \(\bigcap_{n \geq 1} A_n = \emptyset\) then \(\P(A_n) \to 0\) as \(n \to \infty\).
% \end{enumerate}


\begin{definition}[probability measure, probability space]\index{probability measure}\index{probability space}
  Let \(\Omega\) be a set and \(\mathcal F\) a \(\sigma\)-algebra on \(\Omega\). A measure \(\mu\) on \((\Omega, \mathcal F)\) is called a \emph{probability measure} if \(\mu(\Omega) = 1\) and the measure space \((\Omega, \mathcal F, \mu)\) is called a \emph{probability space}.
\end{definition}

\begin{definition}[random variable]\index{random variable}
  A measurable function \(X: \Omega \to \R\) is called a \emph{random variable}.
\end{definition}

We usually use a capital letter to denote a random variable.

\begin{definition}[expectation]\index{expectation}
  If \((\Omega, \mathcal F, \P)\) is a probability space then the \(\P\)-integral is called \emph{expectation}, denoted \(\E\).
\end{definition}

\begin{definition}[distribution/law]\index{distribution}\index{law}
  A random variable \(X: \Omega \to \R\) on a probability space \((\Omega, \mathcal F, \P)\) determines a Borel measure \(\mu_X\) on \(\R\) defined by
  \[
    \mu_X((-\infty, t]) = \P(X \leq t) = \P(\{\omega \in \Omega: X(\omega) \leq t\})
  \]
  and \(\mu_X\) is called the \emph{distribution} of \(X\), or the \emph{law} of \(X\).
\end{definition}

\begin{note}
  \(\mu_X\) is the image of \(\P\) under
  \begin{align*}
    \Omega &\to \R \\
    \omega &\mapsto X(\omega)
  \end{align*}
\end{note}

\begin{definition}[distribution function]\index{distribution function}
  The function
  \begin{align*}
    F_X: \R &\to [0, 1] \\
    t &\mapsto \P(X \leq t)
  \end{align*}
  is called the \emph{distribution function} of \(X\).
\end{definition}

\begin{proposition}\leavevmode
  If \((\Omega, \mathcal F, \P)\) is a probability space and \(X: \Omega \to \R\) is a random variable then \(F_X\) is non-decreasing, right-continuous and it determines \(\mu_X\) uniquely.
\end{proposition}

\begin{proof}
  Given \(t_n \downarrow t\),
  \[
    F_X(t_n) = \P(X \leq t_n) \to \P(\bigcap_{n \geq 1} \{X \leq t_n\}) = \P(\{X \leq t\}) = F_X(t)
  \]
  by downward monotone convergence for sets.
  
  Uniqueness follows from Dynkin lemma applied to the \(\pi\)-system \(\emptyset \cup \{(-\infty, t]\}_{t \in \R}\).
\end{proof}

Conversely,
\begin{proposition}
  If \(F: \R \to [0, 1]\) is a non-decreasing right-continuous function with
  \begin{align*}
    \lim_{t \to -\infty} F(t) &= 0 \\
    \lim_{t \to +\infty} F(t) &= 1
  \end{align*}
  then there exists a unique probability measure \(\mu\) on \(\R\) such that
  \[
    F(t) = \mu((-\infty, t])
  \]
  for all \(t \in \R\).
\end{proposition}

\begin{remark}
  The measure \(\mu\) is called the Lebesgue-Stieltjes measure on \(\R\) associated to \(F\). Furthermore for all \(a, b \in \R\),
  \[
    \mu((a, b]) = F(b) - F(a).
  \]
  We can also construct Lebesgue measure this way.
\end{remark}

\begin{proof}
  Uniqueness is the same as above. For existence, we use the lemma
  \begin{lemma}
    Let
    \begin{align*}
      g: (0, 1) &\to \R \\
      y &\mapsto \inf \{x \in \R: F(x) \geq y\}
    \end{align*}
    then \(g\) is non-decreasing, left-continuous and for all \(x \in \R, y \in (0, 1)\), \(g(y) \leq x\) if and only if \(F(x) \geq y\).
  \end{lemma}

  \begin{proof}
    Let
    \[
      I_y = \{x \in \R: F(x) \geq y\}.
    \]
    Clearly if \(y_1 \geq y_2\) then \(I_{y_1} \subseteq I_{y_2}\) so \(g(y_2) \leq g(y_1)\) so \(g\) is non-decreasing. \(I_y\) is an interval of \(\R\) because if \(x > x_1\) and \(x_1 \in I_y\) then \(F(x) \geq F(x_1) \geq y\) so \(x \in I_y\). So \(I_y\) is an interval with endpoints \(g(y)\) and \(+ \infty\). But \(F\) is right-continuous so \(g(y) = \min I_y\) and the minimum is obtained. Thus \(I_y = [g(y), + \infty)\).

    This means that \(x \geq g(y)\) if and only if \(x \in I_y\) if and only if \(F(x) \geq y\).

    Finally for left-continuity, suppose \(y_n \uparrow y\) then \(\bigcap_{n \geq 1} I_{y_n} = I_y\) by definition of \(I_y\) so \(g(y_n) \to g(y)\).
  \end{proof}
  \begin{remark}
    If \(F\) is continuous and strictly increasing then \(g = F^{-1}\).
  \end{remark}

  Now back to the proposition. Set \(\mu = g_* m\) where \(m\) is the Lebesgue measure on \((0, 1)\), i.e.
  \[
    \mu(A) = m(g^{-1}(A))
  \]
  for all \(A \in \mathcal B(\R)\). \(\mu\) is a probability measure as \(g\) is Borel measure. By the lemma
  \[
    \mu((a, b]) = m(g^{-1}(a, b]) = m((F(a), F(b)) = F(b) - F(a).
  \]
\end{proof}

\begin{proposition}
  If \(\mu\) is a Borel probability measure on \(\R\) then there exist some probability space \((\Omega, \mathcal F, \P)\) and a random variable \(X\) on \(\Omega\) such that \(\mu_X = \mu\).

  In fact, one can even pick \(\Omega = (0, 1)\), \(\mathcal F = \mathcal B(0, 1)\) and \(\P = m\), the Lebesgue measure.
\end{proposition}

\begin{proof}
  Set \(\Omega = \R\), \(\mathcal F = \mathcal B(\R)\), \(\P = \mu\) and \(X(x) = x\).

  Set \(F(t) = \mu((-\infty, t])\) and take \(X = g\) where \(g\) is the auxillary function defined in the previous lemma, namely
  \[
    X(\omega) = \inf\{x: F(x) \geq \omega\}.
  \]
  Check that \(\mu_X = \mu\):
  \begin{align*}
    \mu_X((a, b])
    &= \P(X \in (a, b]) \\
    &= m(\{\omega \in (0, 1): a < X(w) \leq b\}) \\
    &= m(\{\omega \in (0, 1): F(a) < \omega < F(b)\})
  \end{align*}
\end{proof}

\begin{remark}
  If \(\mu\) is a Borel probability measure on \(\R\) such that \(\mu = f dt\) for some \(f \geq 0\) measurable, we say that \(\mu\) has a \emph{density}\index{density} (w.r.t. Lebesgue measure) and \(f\) is called the \emph{density of \(\mu\)}. \(\mu = f dt\) means that \(\mu((a, b]) = \int_a^b f(t) dt\).
\end{remark}

\begin{eg}\leavevmode
  \begin{enumerate}
  \item uniform distribution on \([0, 1]\):
    \begin{align*}
      f(t) &= \mathbf 1_{[0, 1]}(t) \\
      F(t) &= \mu((-\infty, t])
    \end{align*}
  \item exponential distribution of rate \(\lambda\):
    \begin{align*}
      f_\lambda(t) &= \lambda e^{-\lambda t} \mathbf 1_{t \geq 0} \\
      F_\lambda(t) &= \int_{-\infty}^t f_\lambda(t) dt = \mathbf 1_{t \geq 0} (1 - e^{-t/\lambda}) \\
      \end{align*}
    \item Guassian distribution with standart deviation \(\sigma\) and mean \(m\):
      \begin{align*}
        f_{\sigma, m}(t) &= \frac{1}{\sqrt{2\pi \sigma^2}} \exp - \frac{(t - m)^2}{2\sigma^2} \\
        F_{\sigma, m}(t) & = \int_{-\infty}^t \dots
      \end{align*}
  \end{enumerate}
\end{eg}

\begin{definition}[mean, moment, variance]\index{mean}\index{moment}\index{variance}
  If \(X\) is a random variable then
  \begin{enumerate}
  \item \(\E(X)\) is called the \emph{mean},
  \item \(\E(X^k)\) is called the \emph{\(k\)th-moment} of \(X\), which only makes sense when \(X\) is integrable,
  \item \(\var(X) = \E((X - \E X)^2) = \E(X^2) - \E(X)^2\) is called the \emph{variance}.
  \end{enumerate}
\end{definition}

\begin{remark}
  Suppose \(f \geq 0\) measurable and \(X\) is a random variable. Then
  \[
    \E(f(x)) = \int_\R f(x) d \mu_X(x)
  \]
  by definition of \(\mu_X = X_* \P\).
\end{remark}

\section{Independence}

Independence is the key notion that makes probability theory different from (abstract) measure theory.

\begin{definition}[independence]\index{independence}
  Let \((\Omega, \mathcal F, \P)\) be a probability space. A sequence of events \((A_n)_{n \geq 1}\) is called \emph{independent} or \emph{mutually independent} if for all \(F \subseteq \N\) finite,
  \[
    \P(\bigcap_{i \in F} A_i) = \prod_{i \in F} \P(A_i).
  \]
\end{definition}

\begin{definition}[independent \(\sigma\)-algebra]\index{\(\sigma\)-algebra!independence}
  A sequence of \(\sigma\)-algebras \((\mathcal A_n)_{n \geq 1}\) where \(\mathcal A_n \subseteq \mathcal F\) is called \emph{independent} if for all \(A_n \in \mathcal A_n\), the family \((A_n)_{n \geq 1}\) is independent.
\end{definition}

\begin{remark}\leavevmode
  \begin{enumerate}
  \item To prove that \((\mathcal A_n)_{n \geq 1}\) is an independent family, it is enough to check the independence condition for all \(A_n\)'s with \(A_n \in \Pi_n\) where \(\Pi_n\) is a \(\pi\)-system generating \(\mathcal A_n\). The proof is an application of Dynkin lemma. For example for \(\sigma\)-algebras \(\mathcal A_1, \mathcal A_2\),
    \[
      \P(A_1 \cap A_2) = \P(A_1) \P(A_2)
    \]
    for all \(A_1 \in \Pi_1, A_2 \in \Pi_2\). Look at the measures
    \begin{align*}
      A &\mapsto \P(A \cap A_2) \\
      A &\mapsto \P(A) \P(A_2)
    \end{align*}
    on \(\mathcal A_1\). They coincide on \(\Pi_1\) by assumption and hence everywhere on \(\mathcal A_1\). Dually this holds for \(\mathcal A_2\).
  \end{enumerate}
\end{remark}

\begin{notation}
  Suppose \(X\) is a random variable. Denote by \(\sigma(X)\) the smallest \(\sigma\)-subalgebra \(\mathcal A\) of \(\mathcal F\) such that \(X\) is \(\mathcal A\)-measurable, i.e.
  \[
    \sigma(X) = \sigma(\{\omega \in \Omega: X(\omega) \leq t\}_{t \in \R}).
  \]
\end{notation}
% terminal object?

\begin{definition}[independence]\index{random variable!independence}
  A sequence of random variables \((X_i)_{i \geq 1}\) is called \emph{independent} if the sequence of \(\sigma\)-subalgebras \((\sigma(X_i))_{i \geq 1}\) is independent.
\end{definition}

\begin{remark}
  This is equivalent to the condition that for all \((t_i)_{i \geq 1}\), for all \(n\),
  \[
    \P((X_1 \leq t_1) \wedge \dots \wedge (X_n \leq t_n)) = \prod_{i = 1}^n \P(X_i \leq t_i).
  \]

  Yet another equivalent formulation is
  \[
    \mu_{(X_1, \dots, X_n)} = \bigotimes_{i = 1}^n \mu_{X_i}
  \]
  as Borel probability measures on \(\R^n\). ``the joint law is the same as the product of individual laws''
\end{remark}

\begin{note}
  Note that independence is a property of a \emph{family} so pairwise independence is necessary but not sufficient for independence. A famous counterexample is \emph{Berstein's example}: take \(X\) and \(Y\) to be random variables for two independent fair coins flips. Set \(Z = |X - Y|\). Then \(Z = 0\) if and only if \(X = Y\). Check that
  \[
    \P(Z = 0) = \P(Z = 1) = \frac{1}{2}
  \]
  and each pair \((X, Y), (X, Z)\) and \((Y, Z)\) is independent. But \((X, Y, Z)\) is not independent.
\end{note}

\begin{proposition}
  If \(X\) and \(Y\) are independent random variables, \(X \geq 0, Y \geq 0\) then
  \[
    \E(XY) = \E(X) \E(Y).
  \]
\end{proposition}

\begin{proof}
  Essentially Tonelli-Fubini:
  \begin{align*}
    \E(XY) &= \int_{\R^2} xy d\mu_{X, Y} (x, y) = \int_{\R^2} d \mu_X(x) d \mu_Y(y) \\
           &= \left(\int_\R x d \mu_X(x) \right) \left(\int_\R y d \mu_Y(y) \right) \\
           &= \E(X) \E(Y)
  \end{align*}
\end{proof}

\begin{remark}
  As in Tonelli-Fubini, we may require \(XY\) to be integrable instead and the same conclusion holds.
\end{remark}

\begin{eg}
  Let \(\Omega = (0, 1), \mathcal F = \mathcal B(0, 1), \P = m\) the Lebesgue measure. Write the decimal expansion of \(\omega \in (0, 1)\) as
  \[
    \omega = 0. \varepsilon_1 \varepsilon_2 \dots
  \]
  where \(\varepsilon_i(\omega) \in \{0, \dots, 9\}\). Choose a convention so that each \(\omega\) has a well-defined expansion (to avoid things like \(0.099 \dots = 0.100\dots\)). Now let \(X_n(\omega) = \varepsilon_n(\omega)\). Claim that the \((X_n)_{n \geq 1}\) are iid.\ random variables uniformly distributed on \(\{0, \dots, 9\}\), where ``iid'' stands for independent identically distributed.

  \begin{proof}
    Easy check. For example \(X_1(\omega) = \floor{10 \omega}\) so
    \[
      \P(X_1 = i_1) = \frac{1}{10}.
    \]
    Similarly for all \(n\)
    \[
      \P(X_1 = i_1, \dots, X_n = i_n) = \frac{1}{10^n}, \quad \P(X_n = i_n) = \frac{1}{10}
    \]
    so
    \[
      \P(X_1 = i_1, \dots X_n = i_n) = \prod_{i = 1}^n \P(X_k = i_k).
    \]
  \end{proof}
\end{eg}

\begin{remark}
  \[
    \omega = \sum_{n \geq 1} \frac{X_n(\omega)}{10^n}
  \]
  is distributed according to Lebesgue measure so if we want we can construct Lebesgue measure as the law of this random variable.
\end{remark}

\begin{proposition}[infinite product of product measure]
  Let \((\Omega_i, \mathcal F_i, \mu_i)_{i \geq 1}\) be a sequence of probability spaces and let \(\Omega = \prod_{i \geq 1} \Omega_i\) and let \(\mathcal E\) be the Boolean algebra of cylinder sets, i.e.\ sets of the form
  \[
    A \times \prod_{i \geq n} \Omega_i
  \]
  for some \(A \in \bigotimes_{i = 1}^n \mathcal F_i\). Set \(\mathcal F = \sigma(\mathcal E)\), the product \(\sigma\)-algebra. Then there is a unique probability measure \(\mu\) on \((\Omega, \mathcal F)\) such that it agrees with product measures on all cylinder sets, i.e.
  \[
    \mu(A \times \prod_{i > n} \Omega_i) = (\bigotimes_{i = 1}^n \mu_i)(A)
  \]
  for all \(A \in \bigotimes_{i = 1}^n \mathcal F_i\).
\end{proposition}

\begin{proof}
  Omitted. See example sheet 3.
\end{proof}

\begin{lemma}[Borel-Cantelli]\index{Borel-Cantelli lemma}
  Let \((\Omega, \mathcal F, \P)\) be a probability space and \((A_n)_{n \geq 1}\) a sequence of events.
  \begin{enumerate}
  \item If \(\sum_{n \geq 1} \P(A_n) < \infty\) then
    \[
      \P(\limsup_n A_n) = 0.
    \]
  \item Conversely, if \((A_n)_{n \geq 1}\) are independent and \(\sum_{n \geq 1} \P(A_n) = \infty\) then
    \[
      \P(\limsup_n A_n) = 1.
    \]
  \end{enumerate}
\end{lemma}

Note that \(\limsup_n A_n\) is also called \(A_n\) io. ``infinitely often''
% actually only pairwise independent is need in part 2

\begin{proof}\leavevmode
  \begin{enumerate}
  \item Let \(Y = \sum_{n \geq 1} \mathbf 1_{A_n}\) be a random variable. Then
    \[
      \E(Y) = \sum_{n \geq 1} \E(\mathbf 1_{A_n}) = \sum_{n \geq 1} \P(A_n).
    \]
    Since \(Y \geq 0\), recall that we prove that \(\E(Y) < \infty\) implies that \(Y < \infty\) almost surely, i.e.\ \(\P\)-almost everywhere.
  \item Note that
    \[
      (\limsup_n A_n)^c = \bigcup_N \bigcap_{n \geq N} A_n^c
    \]
    so
    \begin{align*}
      \P(\bigcap_{n \geq N} A_n^c)
      &\leq \P(\bigcap_{n = N}^M A_n^c) \\
      &= \prod_{n = N}^M \P(A_n^c)
      = \prod_{n = N}^M (1 - \P(A_n)) \\
      &\leq \prod_{n = N}^M \exp (-\P(A_n)) \\
      &\leq \exp (- \sum_{n = N}^M \P(A_n)) \\
      &\to 0
    \end{align*}
    as \(M \to \infty\). Thus
    \[
      \P(\bigcap_{n \geq N} A_n^c) = 0
    \]
    for all \(N\) so
    \[
      \P(\bigcup_N \bigcap_{n \geq N} A_n^c) = 0.
    \]
  \end{enumerate}
\end{proof}

\begin{definition}[random/stochastic process, filtration, tail \(\sigma\)-algebra, tail event]\index{random process}\index{random process}\index{filtration}\index{\(\sigma\)-algebra!tail}\index{tail event}
  Let \((\Omega, \mathcal F, \P)\) be a probability space and \((X_n)_{n \geq 1}\) a sequence of random variables.
  \begin{enumerate}
  \item \((X_n)_{n \geq 1}\) is sometimes called a \emph{random process} or \emph{stochastic process}.
  \item
    \[
      \mathcal F_n = \sigma(X_1, \dots X_n) \subseteq \mathcal F
    \]
    is called the associated \emph{filtration}. \(\mathcal F_n \subseteq \mathcal F_{n + 1}\).
  \item
    \[
      \mathcal C = \bigcap_{n \geq 1} \sigma(X_n, X_{n + 1}, \dots)
    \]
    is called the \emph{tail \(\sigma\)-algebra} of the process. Its elements are called \emph{tail events}.
  \end{enumerate}
\end{definition}

\begin{eg}
  Tail events are those not affected by the first few terms in the sequence of random variables.
  \[
    \{\omega \in \Omega: \lim_n X_n(\omega) \text{ exists}\}
  \]
  is a tail event, so is
  \[
    \{\omega \in \Omega: \limsup_n X_n(\omega) \geq T\}.
  \]
\end{eg}

\begin{theorem}[Kolmogorov \(0 - 1\) law]\index{Kolmogorov \(0 - 1\) law}
  If \((X_n)_{n \geq 1}\) is a sequence of mutually independent random variables then for all \(A \in \mathcal C\),
  \[
    \P(A) = \{0, 1\}.
  \]
\end{theorem}

\begin{proof}
  Pick \(A \in \mathcal C\). Fix \(n\). For all \(B \in \sigma(X_1, \dots, X_n)\),
  \[
    \P(A \cap B) = \P(A) \P(B)
  \]
  as \(\mathcal C\) is independent of \(\sigma(X_1, \dots, X_n)\). The measures \(B \mapsto \P(A) \P(B)\) and \(B \mapsto \P(A \cap B)\) coincide on each \(\mathcal F_n\) so on \(\bigcup_{n \geq 1} \mathcal F_n\). Hence they coincide on \(\sigma(\bigcup_{n \geq 1} \mathcal F_n) \supseteq \mathcal C\) so
  \[
    \P(A \cap A) = \P(A) \P(A)
  \]
  so
  \[
    P(A) \in \{0, 1\}.
  \]
\end{proof}

\subsection{Useful inequalities}

\begin{proposition}[Cauchy-Schwarz]
  Suppose \(X, Y\) are random variables then
  \[
    \E(|XY|) \leq \sqrt{\E(X^2) \cdot \E(Y^2)}.
  \]
\end{proposition}

\begin{proof}
  For all \(t \in \R\),
  \[
    0 \leq \E((|X| + t|Y|)^2) = \E(X^2) + 2t \E(|XY|) + t^2\E(Y^2)
  \]
  so viewed as a quadratic in \(t\), the discriminant is nonpositive, i.e.
  \[
    (\E(|XY|)^2 - \E(X)^2 - \E(Y)^2 \leq 0.
  \]
\end{proof}

\begin{proposition}[Markov]
  Let \(X \geq 0\) be a random variable. Then for all \(t \geq 0\),
  \[
    t \P(X \geq t) \leq \E(X).
  \]
\end{proposition}

\begin{proof}
  \[
    \E(X)
    \geq \E(X \mathbf 1_{X \geq t})
    \geq \E(t \mathbf 1_{X \geq t})
    = t \P(X \geq t)
  \]
\end{proof}

\begin{proposition}[Chebyshev]
  Let \(Y\) be a random variable with \(\E(Y^2) < \infty\), then for all \(t \in \R\),
  \[
    t^2 \P(|Y - \E(Y)| \geq t) \leq \var Y.
  \]
\end{proposition}

\(\E(Y^2) < \infty\) implies that \(\E(|Y|) < \infty\) by Cauchy-Schwarz, so \(\var Y < \infty\). The reverse is more subtle.

\begin{proof}
  Apply Markov to \(X = |Y - \E(Y)|^2\).
\end{proof}

\begin{theorem}[strong law of large numbers]\index{strong law of large numbers}
  Let \((X_n)_{n \geq 1}\) be a sequence of iid random variables. Assume \(\E(|X_1|) < \infty\). Let
  \[
    S_n = \sum_{k = 1}^n X_k,
  \]
  then \(\frac{1}{n} S_n\) converges almost surely to \(\E(X_1)\).
\end{theorem}

\begin{proof}
  We prove the theorem under a stonger condition: we assume \(\E(|X_1|^4) < \infty\). This implies, by Cauchy-Schwarz, \(\E(|X_1|^2), \E(|X_1|) < \infty\). Subsequently \(\E(|X_1|^3) < \infty\).

  wlog we may assume \(\E(X_1) = 0\), by replacing \(X_n\) by \(X_n - \E(X_1)\). Have
  \begin{align*}
    \E(S_n^4)
    &= \sum_{i,j, k, \ell} \E(X_iX_jX_kX_\ell)
  \end{align*}
  there terms vanish because \(\E(X_i) = 0\) and \((X_i)_{i \geq 1}\) are independnet, except for \(\E(X_i^4)\) and \(\E(X_i^2X_j^2)\) for \(i \neq j\). For example,
  \[
    \E(X_iX_j^3) = \E(X_i) \cdot \E(X_j^3) = 0
  \]
  for \(i \neq j\).
  % symmetric polynomials?

  Thus
  \[
    \E S_n^4 = \sum_{i = 1}^n \E (X_i^4) + 6 \sum_{i < j} \E(X_i^2 X_j^2).
  \]
  By Cauchy-Schwarz,
  \[
    \E(X_i^2X_j^2) \leq \sqrt{\E X_i^4 \cdot \E X_j^4} = \E X_1^4
  \]
  so
  \[
    \E(S_n^4) \leq (n + 6 \cdot \frac{n(n - 1)}{2}) \E X_1^4
  \]
  so
  \[
    \E (\frac{S_n}{n})^4 = O(\frac{1}{n^2})
  \]
  so
  \[
    \E (\sum_{n \geq 1} (\frac{S_n}{n})^4) = \sum_{n \geq 1} \E(\frac{S_n}{n})^4 < \infty.
  \]
  Hence \(\sum (\frac{S_n}{n})^4 < \infty\) almost surely and it follows that
  \[
    \lim_{n \to \infty} \frac{S_n}{n} = 0
  \]
  almost surely.
\end{proof}

statistical implication: we can sample the mean of larger number of iid to detect an unknown law, at least the mean.

We will give a full proof later.

\section{Convergence of random variables}

\begin{definition}[weak convergence]\index{weak convergence}
  A sequence of probability measures \((\mu_n)_{n \geq 1}\) on \((\R^d, \mathcal B(\R^d))\) is said to \emph{converge weakly} to a measure \(\mu\) if for all \(f \in C_b(\R^d)\), the set of continuous bounded functions on \(\R^d\),
  \[
    \lim_{n \to \infty} \mu_n(f) = \mu(f).
  \]
\end{definition}

\begin{eg}\leavevmode
  \begin{enumerate}
  \item Let \(\mu_n = \delta_{1/n}\) be the Dirac mass on \(\R^n\), where for \(x \in \R^d\), \(\delta_x\) is the probability on \(\R^d\) such that
    \[
      \delta_x(A) =
      \begin{cases}
        1 & x \in A \\
        0 & x \notin A
      \end{cases}
    \]
    then \(\mu_n \to \delta_0\).
  \item Let \(\mu_n = \mathcal N(0, \sigma_n^2)\), Gaussian distribution with standard deviation \(\sigma_n\), where \(\sigma_n \to 0\), then again \(\mu_n \to \delta_0\). Indeed,
    \[
      \mu_n(f)
      = \int f(x) d\mu(x)
      = \int f(x) \frac{1}{\sqrt{2\pi \sigma_n^2}} \exp (- \frac{x^2}{2\pi_n^2}) dx
      = \int f(x\sigma_n) \frac{1}{\sqrt{2\pi}} \exp - \frac{x^2}{2} dx
    \]
    \(\sigma_n \to 0\) so \(f(\sigma_n x) \to f(0)\) so by DCT, \(\mu_f(f) \to f(0) = \delta_0(f)\).
  \end{enumerate}
\end{eg}

\begin{definition}[convergence of random variable]\index{convergence!almost surely}\index{convergence!in probability}\index{convergence!in measure}\index{convergence!in distribution}
  A sequence \((X_n)_{n \geq 1}\) of \(\R^d\)-valued random variables on \((\Omega, \mathcal F, \P)\) is said to converge to a random variable \(X\) if
  \begin{enumerate}
  \item \emph{almost surely} if
    \[
      \lim_{n \to \infty} X_n(\omega) = X(\omega)
    \]
    for \(\P\)-almost every \(\omega\).
  \item \emph{in probability} or \emph{in measure} if for all \(\varepsilon > 0\),
    \[
      \lim_{n \to \infty} \P(\norm{X_n - X} > \varepsilon) = 0.
    \]
    Note that all norms on \(\R^d\) are equivalent so we don't have to specify one.
  \item \emph{in distribution} or \emph{in law} if \(\mu_{X_n} \to \mu_X\) weakly, where \(\mu_X = X_*\P\) is the law of \(X\), a Borel probability measure on \(\R^d\). Equivalently, for all \(f \in C_b(\R^d)\), \(\E(f(X_n)) \to \E(f(X))\).
  \end{enumerate}
\end{definition}

\begin{proposition}
  \(1 \implies 2 \implies 3\).
\end{proposition}

\begin{proof}\leavevmode
  \begin{enumerate}
  \item \(1 \implies 2\):
    \[
      \P(\norm{X_n - X} > \varepsilon) = \E(\mathbf 1_{\norm{X_n - X} > \varepsilon})
    \]
    so if \(X_n \to X\) almost surely for \(\P\)-almost everywhere \(\omega\),
    \[
      \mathbf 1_{\norm{X_n - X} > \varepsilon} (\omega) \to 0
    \]
    so by dominated convergence theorem \(\dots \to 0\).
  \item \(2 \implies 3\): given \(f \in C_b(\R^d)\), need to show that \(\mu_{X_n}(f) \to \mu_X(f)\). But
    \[
      \mu_{X_n}(f) - \mu_X(f) = \E(f(X_n) - f(x)).
    \]
    To bound this, note that \(f \in C_b(\R^d)\) implies that it is locally uniformly continuous, i.e.\ for all \(\varepsilon > 0\) exists \(\delta > 0\) such that if \(\norm x < \frac{1}{\varepsilon}\) and \(\norm{y - x} < \delta\) then \(|f(y) - f(x)| < \varepsilon\). Thus
    \[
      |\E(f(X_n)  - f(X))|
      \leq \E(\mathbf 1_{\norm{X_n - X} < \delta} \mathbf 1_{\norm x < 1/\varepsilon} |f(X_n) - f(X)|)
      + 2 \norm f_\infty (\P(\norm{X - X_n} > \delta) + \P(\norm X \geq \frac{1}{\varepsilon}))
    \]
    But \(\norm f_\infty < \infty\) so
    \[
      * \leq \varepsilon + 2 \norm f_\infty (\P(\norm{X_n - X} \geq \delta) + \P(\norm X \geq \frac{1}{\varepsilon}))
    \]
    so
    \[
      \limsup_{n \to \infty} *_n \leq \varepsilon + 2 \norm f_\infty \underbrace{P(\norm X > \frac{1}{\varepsilon})}_{\to 0 \text{ as } \varepsilon \to 0}.
    \]
    so \(\limsup = 0\).
  \end{enumerate}
\end{proof}

\begin{remark}
  When \(d = 1\), 3 is equivalent to \(F_{X_n}(x) \to F_X(x)\) for all \(x\) as \(n \to \infty\) where \(F_X\) is continuous. See example sheet 3.
\end{remark}

The converses do not hold.

\begin{proposition}
  If \(X_n \to X\) in probability then there is a subsequence \((n_k)_k\) such that \(X_{n_k} \to X\) almost surely as \(k \to \infty\).
\end{proposition}

\begin{proof}
  We know for all \(\varepsilon > 0\), \(\P(\norm{X_n - X}) \to 0\) as \(n \to \infty\). So for all \(k\) exists \(n_k\) such that
  \[
    \P(\norm{X_{n_k} - X} > \frac{1}{k}) \leq \frac{1}{2^k}
  \]
  so
  \[
    \sum_{k \geq 1} \P(\norm{X_{n_k} - X} > \frac{1}{k}) < \infty
  \]
  so by the first Borel-Cantelli lemma
  \[
    \P(\norm{X_{n_k} - X} > \frac{1}{k} \text{ i.o.}) = 0.
  \]
  This means that with probability \(1\), \(\norm{X_{n_k} - X} \to 0\) as \(k \to \infty\).
\end{proof}

\begin{definition}[convergence in mean]\index{convergence!in mean}\index{convergence!in \(L^1\)}
  Let \((X_n)_{n \geq 1}\) and \(X\) be \(\R^d\)-valued integrable random variables. We say that \((X_n)_n\) \emph{converges in mean} or \emph{in \(L^1\)} to \(X\) if
  \[
    \lim_{n \to \infty} \E(\norm{X_n - X}) = 0.
  \]
\end{definition}

\begin{remark}\leavevmode
  \begin{enumerate}
  \item If \(X_n \to X\) in mean then \(X_n \to X\) in probability: this is because of the Markov inequality
    \[
      \varepsilon \cdot \P(\norm{X_n - X} > \varepsilon) \leq \E(\norm{X_n - X}).
    \]
  \item The converse is false. For example take \(\Omega = (0, 1)\), \(\mathcal F\) to be the Borel \(\sigma\)-algebra and \(\P\) Lebesgue measure. Let \(X_n = n \mathbf 1_{[0, \frac{1}{n}]}\). \(X_n \to 0\) almost surely but \(\E X_n = 1\).
  \end{enumerate}
\end{remark}

When does converge surely imply convergence in mean? We need some kind of domination assumption.

\begin{definition}[uniformly integrable]\index{uniformly integrable}
  A sequence of random variables is called \emph{uniformly integrable} if
  \[
    \lim_{M \to \infty} \limsup_{n \to \infty} \E(\norm{X_n} \mathbf 1_{\norm{X_n} \geq M}) = 0.
  \]
\end{definition}

\begin{remark}
  If \((X_n)_{n \geq 1}\) are \emph{dominated}, namely exists an integrable random variable \(Y \geq 0\) such that \(\norm{X_n} \leq Y\) for all \(n\) then \((X_n)_n\) is uniformly integrable, because of dominated convergence theorem:
  \[
    \E(\norm{X_n} \mathbf 1_{\norm{X_n} \geq M}) \leq \E(Y \mathbf 1_{Y \geq M}) \to 0
  \]
  as \(M \to \infty\).
\end{remark}

\begin{theorem}
  Let \((X_n)_{n \geq 1}\) be a sequence of \(\R^d\)-valued integrable random variable. Let \(X\) be another random variable. Then TFAE:
  \begin{enumerate}
  \item \(X\) is integrable and \(X_m \to X\) in mean,
  \item \((X_n)_n\) is uniformly integrable and \(X_n \to X\) in probability.
  \end{enumerate}
\end{theorem}

\begin{proof}\leavevmode
  \begin{enumerate}
  \item \(1 \implies 2\): By remark we already know \(X_n \to X\) in probability so left to show uniform integrability.
    \begin{align*}
      \E(\norm{X_n} \mathbf 1_{\norm{X_n} \geq M})
      &\leq \E(\norm{X_n - X} \mathbf 1_{\norm{X_n} \geq n}) + \E(\norm{X} \mathbf 1_{\norm{X_n} \geq M}) \\
      &\leq \E(\norm{X_n - X}) + \E(\norm{X} \mathbf 1_{\norm{X_n} \geq M} (\mathbf 1_{\norm X \leq \frac{M}{2}} + \mathbf 1_{\norm X > \frac{M}{2}})) \\
      &\leq \E(\norm{X_n - X}) + \E(\norm X \mathbf 1_{\norm{X_n - X} \geq \frac{M}{2}} \mathbf 1_{\norm X \leq \frac{M}{2}}) + \E(\norm X \mathbf 1_{\norm X \geq \frac{M}{2}}) \\
      &\leq \E(\norm{X_n - X}) + \frac{M}{2} \P(\norm{X_n - X} \geq \frac{M}{2}) + \E(\norm X \mathbf 1_{\norm X \geq \frac{M}{2}})
    \end{align*}
    Take \(\limsup\),
    \begin{align*}
      \limsup_{n \to \infty} \E(\norm{X_n} \mathbf 1_{\norm{X_n} \geq n}
      &\leq 0 + 0 +
        %\E(\norm{X_n - X}) + \frac{M}{2} \P(\norm{X_n - X} \geq \frac{M}{2}) + \E(\norm X \mathbf 1_{\norm X \geq \frac{M}{2}})
    \end{align*}
  \item \(2 \implies 1\): Let's prove that \(X\) is integrable. By the proposition, we can find a subsequence \((n_k)_k\) such that \(X_{n_k} \to X\) almost surely. By Fatou's lemma,
    \[
      \E(\norm X \mathbf 1_{\norm X \geq M}) \leq \liminf_{k \to 0} \E(\norm{X_{n_k}} \mathbf 1_{\norm{X_{n_k} \geq m}})
    \]
    which goes to \(0\) as \(M \to \infty\) by uniform integrability assumption. THus
    \[
      \E(\norm X) \leq M + \E(\norm X \mathbf 1_{\norm X \geq M}) < \infty
    \]
    for \(M\) sufficiently large. Thus \(X\) is integrable.

    To show convergence in mean, we use the same trick of spliting into small and big parts.
    \begin{align*}
      \E(\norm{X_n - X})
      &= \E([\mathbf 1_{\norm{X_n - X} \leq \varepsilon} + \mathbf 1_{\norm{X_n - X} > \varepsilon}] \norm{X_n - X}) \\
      &\leq \varepsilon + \E(\mathbf 1_{\norm{X_n - X} > \varepsilon} \norm{X_n - X} (\mathbf 1_{\norm{X_n} \leq M} + \mathbf 1_{\norm{X_n} > M})) \\
      &\leq \varepsilon + 1 + 2
    \end{align*}
    where
    \begin{align*}
      1
      &= \E(\norm{X_n - X} \mathbf 1_{\norm{X_n - X} > \varepsilon} \mathbf 1_{\norm{X_n} \leq M}) \\
      &\leq \E((M + \norm X) \mathbf 1_{\norm{X_n - X} > \varepsilon} (\mathbf 1_{\norm X \leq M} + \mathbf 1_{\norm X > M}) \\
      &\leq 2M \P(\norm{X_n - X} > \varepsilon) + 2 \E(\norm X \mathbf 1_{\norm X > m})
    \end{align*}
    so
    \[
      \limsup_{n \to \infty} 1 \leq 2 \E(\norm X \mathbf 1_{\norm X > M}) \to 0
    \]
    as \(M \to \infty\).

    On the other hand
    \begin{align*}
      2
      &= \E(\norm{X_n - X} \mathbf 1_{\norm{X_n - X} > \varepsilon} \mathbf 1_{\norm{X_n} > M}) \\
      &\leq \E((\norm{X_n} + \norm X) \mathbf 1_{\norm{X_n} \geq m}) \\
      &\leq \E(\norm{X_n} \mathbf 1_{\norm{X_n} \geq M} + \norm X \mathbf 1_{\norm X > M} + \norm X \mathbf 1_{\norm{X_n} \geq M} \mathbf 1_{\norm X \leq M}) \\
      &\leq 2 \E(\norm{X_n} \mathbf 1_{\norm{X_n} \geq M}) + \E(\norm X \mathbf 1_{\norm X > M})
    \end{align*}
    taking \(\limsup\),
    \[
      \limsup_{n \to \infty} 2 \leq 2 \limsup_{n \to \infty} \E(\norm{X_n} \mathbf 1_{\norm{X_n} \geq M}) + \E(\norm X \mathbf 1_{\norm X > M}) \to 0 + 0
    \]
    as \(M \to \infty\).
  \end{enumerate}
\end{proof}

\begin{definition}
  We say that a sequence of random variables \((X_n)_{n \geq 1}\) is \emph{bounded} in \(L^p\) if there exists \(C > 0\) such that \(\E(\norm{X_n}^p) \leq C\) for all \(n\).
\end{definition}

\begin{proposition}
  If \(p > 1\) and \((X_n)_{n \geq 1}\) is bounded in \(L^p\) then \((X_n)_n\) is uniformly integrable.
\end{proposition}

\begin{proof}
  By Markov inequality,
  \[
    M^{p - 1} \E(\norm{X_n} \mathbf 1_{\norm{X_n} \geq M}) \leq \E( \norm{X_n}^p) \leq C
  \]
  so
  \[
    \limsup_{n \to \infty} \E(\norm{X_n} \mathbf 1_{\norm{X_n} \geq M}) \leq \frac{C}{M^{p - 1}} \to 0
  \]
  as \(M \to \infty\).
\end{proof}
This provides a sufficient condition for uniform integrability and thus convergnce in mean.


















\printindex
\end{document}

% http://www.statslab.cam.ac.uk/~james/Lectures/pm.pdf

% Books: books listed on course handbook, as well as
% Intorduction to measure theory, T. Tao
% Real and Complex analysis, W. Rudin

% related courses: II Linear Analysis

% schedule
% wk 1: Lebesgue measure on \R^d
% wk 2: abstract measure theory
% wk 3: integration
% wk 4: measure theoretical foundations of probability theory
% wk 5: modes of convergence of random variables
% wk 6: Hilbert space techniques, L^p spaces
% wk 7: Fourier analysis, Gaussian random variables, Central Limit Theorem, Law of Large Number Theory
% wk 8: Ergodic Theorem