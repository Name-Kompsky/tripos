\documentclass[a4paper]{article}

\def\npart{IB}

\def\ntitle{Groups, Rings and Modules}
\def\nlecturer{O.\ Randal-Williams}

\def\nterm{Lent}
\def\nyear{2017}

\input{header}

\DeclareMathOperator{\Ann}{Ann}
\DeclareMathOperator{\fit}{Fit}

\makeindex

\begin{document}

\input{titlepage}

\tableofcontents

\section{Groups}

\subsection{Definitions}

\begin{definition}[Group]\index{group}
  A \emph{group} is a triple \((G, \cdot, e)\) of a set \(G\), a function \(- \cdot -: G \times G \to G\) and \(e \in G\) such that
  \begin{itemize}
  \item associativity: for all \(a, b, c \in G\), \((a \cdot b) \cdot c = a \cdot (b \cdot c)\),
  \item identity: for all \(a \in G\), \(a \cdot e = a = e \cdot a\),
  \item inverse: for all \(a \in G\), there exists \(\alpha^{-1} \in G\) such that \(a \cdot a^{-1} = e = a^{-1} \cdot a\).
  \end{itemize}
\end{definition}

\begin{definition}[Subgroup]\index{group!subgroup}
  If \((G, \cdot, e)\) is a group, \(H \subseteq G\) is a \emph{subgroup} if
  \begin{itemize}
  \item \(e \in H\),
  \item for all \(a, b \in H\), \(a \cdot b \in H\).
  \end{itemize}
  This makes \((H, \cdot, e)\) into a group. Write \(H \leq G\).
\end{definition}

\begin{lemma}
  If \(H \subseteq G\) is non-emphy and for all \(a, b \in H\), \(a \cdot b^{-1} \in H\) then \(H \leq G\).
\end{lemma}

\begin{eg}\leavevmode
  \begin{enumerate}
  \item Additive groups: \((\N, +, 0)\), \((\R, +, 0)\), \((\C, +, 0)\).
  \item Groups of symmetries: \(S_n\), \(D_{2n}\), \(\GL_n(\R)\). They have subgroups \(A_n \leq S_n\), \(C_n \leq D_{2n}\), \(\SL_n(\R) \leq \GL_n(\R)\).
  \item An group \(G\) is \emph{abelian}\index{group!abelian} is a group such that \(a \cdot b = b \cdot a\) for all \(a, b \in G\).
  \end{enumerate}
\end{eg}

If \(H \leq G\), \(g \in G\), we define the \emph{left \(H\)-coset} of \(G\) to be
\[
  gH = \{gh: h \in H\}.
\]
As we have seen in IA Groups, the \(H\)-cosets form a patition of \(G\) and are in bijection with each other via
\begin{align*}
  H &\leftrightarrow gH \\
  h &\mapsto gh \\
  g^{-1}h &\mapsfrom h
\end{align*}
We write \(G/H\) for the set of left cosets.

\begin{theorem}[Lagrange]\index{Lagrange's Theorem}
  \label{thm:lagrange}
  If \(G\) is a finite group and \(H \leq G \) then
  \[
    |G| = |H| \cdot |G/H|.
  \]
\end{theorem}

We call \(|G/H|\) the \emph{index}\index{group!index} of \(H\) in \(G\).

\begin{definition}[Order]\index{order}
  Given \(g \in G\), the \emph{order} of \(g\) is the smallest \(n\) such that \(g^n = e\). We write \(n = o(g) = |g|\). If no such \(n\) exists then \(g\) has infinite order.
\end{definition}

Recall that if \(g^m = e\) then \(|g| \divides m\).

\begin{lemma}
  If \(G\) is finite and \(g \in G\) then \(|g| \divides |G|\).
\end{lemma}

\begin{proof}
  The set
  \[
    \generation g = \{e, g, \dots, g^{|g| - 1}\}
  \]
  is a subgroup of \(G\). The result follows from \nameref{thm:lagrange}.
\end{proof}

\subsection{Normal subgroups, Quotients and Homomorphisms}

Recall that \(gH = g'H\) if and only if \(g^{-1}g' \in H\). In particular, if \(h \in H\) then \(ghH = gH\).

Given a subgroup \(H \leq G\), we want to define a group structure on its cosets. Argurably the most natural candidate for the group operation would be
\begin{align*}
  - \cdot -: G/H \times G &\to G/H \\
  (gH, g'H) &\mapsto gg'H
\end{align*}
But is this well-defined? Suppose \(g'H = g'hH\), then
\[
  (gH) \cdot (g'hH) = gg'hH = gg'H
\]
so it is well-defined in the second coordinate. Suppose \(gH = ghH\), then
\[
  (ghH) \cdot (g'H) = ghg' H \stackrel{?}{=} gg'H
\]
where the last step holds if and only if \((g')^{-1}hg' \in H\) for all \(h \in H\), \(g' \in G\). Thus we need this true to define a group structure on the cosets. This motivates us to define

\begin{definition}[Normal subgroup]\index{group!subgroup!normal}
  A subgroup \(H \leq G\) is \emph{normal} if for all \(h \in H\), \(g \in G\), \(g^{-1}hg \in H\). Write \(H \normal G\).
\end{definition}

\begin{definition}[Quotient group]\index{group!quotient}
  If \(H \normal G\), then \(G/H\) equipped with the product
  \begin{align*}
    G/H \times G/H &\to G/H \\
    (gH, g'H) &\mapsto gg'H
  \end{align*}
  and identity \(eH\) is a group. This is the \emph{quotient group} of \(G\) by \(H\).
\end{definition}

Now we have defined and seen quite a few groups. We are interested not in the internal structure of groups but how they relate to each other. This motivates to define morphisms between groups:

\begin{definition}[Homomorphism]\index{group!homomorphism}
  If \((G, \cdot, e_G)\) and \((H, *, e_H)\) are groups, a function \(\varphi: G \to H\) is a \emph{homomorphism} if for all \(g, h \in G\),
  \[
    \varphi(g \cdot g') = \varphi(g) * \varphi(g').
  \]
\end{definition}

This implies that \(\varphi(e_G) = e_H\) and \(\varphi(g^{-1}) = \varphi(g)^{-1}\). We define
\begin{align*}
  \ker \varphi &= \{g \in G: \varphi(g) = e_H\}, \\
  \im \varphi &= \{\varphi(g): g \in G\}.
\end{align*}

\begin{lemma}\leavevmode
  \begin{itemize}
  \item \(\ker \varphi \normal G\),
  \item \(\im \varphi \leq H\).
  \end{itemize}
\end{lemma}

\begin{proof}
  Easy.
\end{proof}

\begin{definition}[Isomorphism]\index{group!isomorphism}
  A homomorphism \(\varphi\) is an \emph{isomorphism} if it is a bijection. Say \(G\) and \(H\) are \emph{isomorphic} if there exists some isomorphism \(\varphi: G \to H\). Write \(G \cong H\).
\end{definition}

\begin{ex}
  If \(\varphi\) is an isomorphism then the inverse \(\varphi^{-1}: H \to G\) is also an isomorphism.
\end{ex}

\begin{theorem}[First Isomorphism Theorem]\index{tbd}
  Let \(\varphi: G \to H\) be a homomorphism . Then \(\ker \varphi \leq G\), \(\im \varphi \leq G\) and
  \[
    G/\ker \varphi \cong \im \varphi.
  \]
\end{theorem}

\begin{proof}
  We have done the first part. For the second part, define
  \begin{align*}
    \theta: G/\ker \varphi &\to \im \varphi \\
    g\ker \varphi &\mapsto \varphi(g)
  \end{align*}
  \[
    \begin{tikzcd}
      G \ar[r, "\varphi"] \ar[d, two heads] & H \\
      G/\ker \varphi \ar[ur, dashed, "\phi"']
    \end{tikzcd}
  \]
  Check this is well-defined: if \(g\ker \varphi = g'\ker \varphi\) then \(g^{-1}g' \in \ker \varphi\) so \(e_H = \varphi(g^{-1}g') = \varphi(g)^{-1}\varphi(g')\), \(\varphi(g) = \varphi(g')\) and \(\theta(g\ker \varphi) = \theta(g'\ker \varphi)\).

  \(\theta\) is a homomorphism:
  \[
    \theta(g\ker \varphi \cdot g'\ker \varphi) = \theta(gg' \ker \varphi) = \varphi(gg') = \varphi(g)\varphi(g') = \theta(g\ker \varphi) \theta(g\ker \varphi).
  \]
  \(\theta\) is surjective and finally to show it is injective, suppose \(\theta(g\ker \varphi) = e_H\). Then \(g \in \ker \varphi\) so \(g\ker \varphi = e\ker \varphi\).
\end{proof}

\blindtext

\section{Rings}

\blindtext

\section{Modules}

\subsection{Definitions}

\begin{definition}[Module]\index{module}
  Let \(R\) be a commutative ring. A quadruple \((M, +, , 0_M, \cdot)\) is an \emph{\(R\)-module} if \((M, +, 0_M)\) is an abelian group and the operation \(- \cdot -: R \times M \to M\) satisfies
  \begin{itemize}
  \item \((r_1 + r_2) \cdot m = r_1 \cdot m + r_2 \cdot m\),
  \item \(r \cdot (m_1 + m_2) = r \cdot m_1 + r \cdot m_2\),
  \item \(r_2 \cdot (r_1 \cdot m) = (r_2r_2) \cdot m\),
  \item \(1_R \cdot m = m\).
  \end{itemize}
\end{definition}

\begin{eg}\leavevmode
  \begin{enumerate}
  \item If \(R = \F\) is a field then an \(\R\)-module is precisely an \(\F\)-vector space.
  \item For any ring \(R\), \(R^n = \underbrace{R \times \dots \times R}_{n \text{ times}}\) is an \(\R\)-module via
    \[
      r \cdot (r_1, \dots, r_n) = (rr_1, \dots, rr_n).
    \]
    In particular for \(n = 1\), \(R\) is an \(R\)-module.
  \item If \(I \ideal R\) then \(I\) is an \(R\)-module via
    \[
      r \cdot a = ra \in I.
    \]
    Also \(R/I\) is an \(\R\)-module via
    \[
      r \cdot (r_1 + I) = rr_1 + I \in R/I.
    \]
  \item For \(R = \Z\), an \(\R\)-module is precisely an abelian group. This is because the axiom for \(\cdot\) says that
    \begin{align*}
      - \cdot -: \Z \times M &\to M \\
      (n, m) &\mapsto
               \begin{cases}
                 \underbrace{m + \dots + m}_{n \text{ times}} & n \geq 0 \\
                 -(\underbrace{m + \dots + m}_{n \text{ times}}) & n < 0 \\
               \end{cases}
    \end{align*}
    so \(\cdot\) is uniquely determined by \(M\).
  \item Let \(\F\) be a field and \(V\) be a vector space over \(\F\). Let \(\alpha: V \to V\) be a linear map. Then we can make \(V\) into an \(\F[X]\)-module via
    \begin{align*}
      \F[X] \times V &\to V \\
      (f, v) &\mapsto f(\alpha)(v)
    \end{align*}
    Different \(\alpha\)'s make \(V\) into different \(\F[x]\)-modules.
  \item Restriction of scalars: if \(\varphi: R \to S\) is a ring homomorphism and \(M\) is an \(S\)-module, then \(M\) becomes an \(R\)-modules via
    \[
      r \cdot_R m = \varphi(r) \cdot_s m.
    \]
  \end{enumerate}
\end{eg}

\begin{definition}[Submodule]\index{module!submodule}
  If \(M\) is an \(\R\)-module, \(N \subseteq M\) is a \emph{submodule} if \(N\) is a subgroup of \((M, +, 0_M)\) and for any \(n \in N, r \in R\), \(r \cdot n \in N\). Write \(N \leq M\).
\end{definition}

\begin{eg}
  A subset of \(R\) is a submodule if and only if it is an ideal.
\end{eg}

\begin{definition}[Quotient module]\index{module!quotient}
  If \(N \leq M\) is a submodule, the \emph{quotient module} \(M/N\) is the set of \(N\)-cosets in \((M, +, 0_M)\), i.e. the quotient abelian group with
  \[
    r \cdot (m + N) = r \cdot m + N.
  \]
\end{definition}

\begin{definition}[Homomorphism]\index{module!homomorphism}
  A function \(f: M \to N\) is an \emph{\(\R\)-module homomorphism} if it is a homomorphism of abelian groups and \(f(r \cdot m) = r \cdot f(m)\).
\end{definition}

\begin{eg}
  If \(R = \F\) is a field and \(V\) and \(W\) are \(\F\)-modules (i.e. \(\F\)-vector spaces), then a map is an \(\F\)-module homomorphism if and only if it is an \(\F\)-linear map.
\end{eg}

\begin{theorem}[First Isomorphism Theorem]\index{tbd}
  If \(f: M \to N\) is an \(R\)-module homomorphism then
  \begin{align*}
    \ker f &= \{m \in M: f(m) = 0\} \leq M \\
    \im f &= \{n \in N: n = f(m)\} \leq N
  \end{align*}
  and
  \[
    M/\ker f \cong \im f.
  \]
\end{theorem}

\begin{theorem}[Second Isomorphism Theorem]\index{tbd}
  Let \(A, B \leq M\) be submodules. Then
  \begin{align*}
    A + B &= \{m \in M: m = a + b, a \in A, b \in B\} \leq M \\
    A \cap B &\leq M
  \end{align*}
  and
  \[
    (A + B)/A \cong B/(A \cap B).
  \]
\end{theorem}

\begin{theorem}[Third Isomorphism Theorem]\index{tbd}
  Let \(N \leq L \leq M\) be a chain of submodules. Then
  \[
    \frac{M/N}{L/N} \cong M/L.
  \]
\end{theorem}

\begin{definition}[Annihilator]\index{annihilator}
  If \(M\) is an \(R\)-module and \(m \in M\), the \emph{annihilator} of \(m\) is
  \[
    \Ann(m) = \{r \in R: r \cdot m = 0_M\} \ideal R.
  \]

  The \emph{annihilator} of \(M\) is
  \[
    \Ann(M) = \bigcap_{m \in M} \Ann(m) \ideal R.
  \]
\end{definition}

\begin{definition}[Generated submodule]\index{module!submodule!generated}
  If \(M\) is an \(R\)-module and \(m \in M\), the \emph{submodule generated by \(m\)} is
  \[
    Rm = \{r \cdot m \in M: r \in R\}.
  \]
\end{definition}

\begin{note}
  Intuitively, the annihilator of an element is the stabiliser of a ring action and that of a module is the kernel. We also have
  \[
    Rm \cong R/\Ann(m).
  \]
\end{note}

\begin{definition}[Finitely generated]\index{finitely generated}
  \(M\) is \emph{finitely generated} if there are \(m_1, \dots, m_n \in M\) such that
  \[
    M = Rm_1 + \dots Rm_n = \{r_1m_1 + \dots + r_nm_r: r_i \in R\}.
  \]
\end{definition}

\begin{lemma}
  An \(R\)-module \(M\) is finitely generated if and only if there is a surjetion \(\varphi: R^n \surj M\) for some \(n\).
\end{lemma}

\begin{proof}\leavevmode
  \begin{itemize}
  \item \(\Rightarrow\): Suppose \(M = Rm_1 + \dots + Rm_n\). Define
    \begin{align*}
      \varphi: R^n &\to M \\
      (r_1, \dots, r_n) &\mapsto r_1m_1 + \dots + r_nm_m
    \end{align*}
    This is an \(R\)-module homomorphism and is surjective.
  \item \(\Leftarrow\): Let \(m_i = \varphi((0, \dots, 0, 1, 0, \dots, 0))\) with \(1\) in the \(i\)th position. Then
    \begin{align*}
      \varphi((r_1, \dots, r_n)) &= \varphi((r_1, 0, \dots, 0) + \dots + (0, \dots, 0, r_n)) \\
                                 &= \varphi((r_1, 0, \dots, 0)) + \dots + \varphi((0, \dots, 0, r_n)) \\
                                 &= r_1 \varphi((1, 0, \dots, 0)) + \dots + r_n \varphi((0, \dots, 0, 1)) \\
      &= r_1m_1 + \dots + r_nm_n
    \end{align*}
    As \(\varphi\) is surjective, \(M = Rm_1 + Rm_n\).
  \end{itemize}
\end{proof}

\begin{corollary}
  Let \(M\) be an \(R\)-module and \(N \leq M\). If \(M\) is finitely generated the so is \(M/N\).
\end{corollary}

\begin{proof}
  \[
    R^n \overset{f}{\surj} M \overset{\pi}{\surj} M/N.
  \]
\end{proof}

\begin{note}
  A module of a finitely generated \(R\)-module need \emph{not} to be finitely generated. For example,
  \[
    (X_1, X_2, \dots) \ideal \Z[X_1, X_2, \dots] = R
  \]
  is an \(R\)-module but note finitely generated, as otherwise it would be a finitely generated ideal.
\end{note}

\begin{eg}
  For \(\alpha \in \C\), \(\alpha\) is an algebraic integer if and only if \(\Z[\alpha]\) is a finitely generated \(\Z\)-module.
\end{eg}

\subsection{Direct Sums and Free Modules}

\begin{definition}[Direct sum]\index{direct sum}
  If \(M_1, \dots, M_k\) are \(R\)-modules, the \emph{direct sum} \(M_1 \oplus \dots \oplus M_k\) is the set \(M_1 \times \dots \times M_k\) with addition
  \[
    (m_1, \dots, m_k) + (m_1', \dots, m_k') = (m_1 + m_1', \dots, m_k + m_k')
  \]
  and \(R\)-module structure
  \[
    r \cdot (m_1, \dots, m_k) = (rm_1, \dots, rm_k).
  \]
\end{definition}

\begin{eg}
  \[
    R^n = \underbrace{R \oplus \dots \oplus R}_{n \text{ times}}.
  \]
\end{eg}

\begin{definition}[Independence]\index{independenc}
  Let \(m_1, \dots m_k \in M\). They are \emph{independent} if
  \[
    \sum_i r_i \cdot m_i = 0
  \]
  implies that \(r_i = 0\) for all \(1 \leq i \leq k\).
\end{definition}

\begin{definition}[Free generation]\index{tbd}
  A subset \(S \subseteq M\) \emph{generates \(M\) freely} if
    \begin{enumerate}
    \item \(S\) generates \(M\).
    \item Any function \(\psi: S \to N\) to an \(R\)-module \(N\) extends to an \(R\)-module homomorphism \(\theta: M \to N\).
    \end{enumerate}
    \[
      \begin{tikzcd}
        S \ar[r, hook] \ar[dr, "\psi"'] & R^S \ar[d, dashed, "\theta"] \\
        & N
      \end{tikzcd}
    \]
\end{definition}

\begin{note}
  We can show this extension is unique: given \(\theta_1, \theta_2: M \to N\) two extensions of \(\psi\), \(\theta_1 - \theta_2:M \to N\) is an \(R\)-module homomorphism so \(\ker(\theta_1 - \theta_2) \leq M\). But \(\theta_1, \theta_2\) both extend \(\psi\) so \(S \subseteq \ker(\theta_1 - \theta_2)\). As \(S\) generates \(M\), \(M \leq \ker(\theta_1 - \theta_2)\) so \(\theta_1 = \theta_2\).
\end{note}

An \(R\)-module which is freely generated by \(S \subseteq M\) is said to be \emph{free} and \(S\) is called a \emph{basis}

\begin{proposition}
  For a finite subset \(S = \{m_1, \dots, m_k\} \subseteq M\), TFAE:
  \begin{enumerate}
  \item \(M\) is freely generated by \(S\).
  \item \(M\) is generated by \(S\) and \(S\) is independent.
  \item Every \(m \in M\) can be written as \(r_1m_1 + \dots + r_km_k\) for some unique \(r_i \in R\).
  \end{enumerate}
\end{proposition}

\begin{proof}\leavevmode
  \begin{itemize}
  \item \(1 \Rightarrow 2\): Let \(S\) generate \(M\) freely. If \(S\) is not independent, then there is a non-trivial relation
    \[
      \sum_{i = 1}^k r_im_i = 0
    \]
    with \(r_j \neq 0\). Let
    \begin{align*}
      \psi: S &\to R \\
      m_i &\mapsto
            \begin{cases}
              0_R & i \neq j \\
              1_R & i = j
            \end{cases}
    \end{align*}
    This extends to an \(R\)-module homomorphism \(\theta: M \to R\). Then
    \[
      0 = \theta(0) = \theta \left(\sum r_im_i \right) = \sum r_i\theta(m_i) = r_j.
    \]
    Absurd. Thus \(S\) is independent.
  \item The other steps follow similarly from those in IB Linear Algebra.
  \end{itemize}
\end{proof}

\begin{eg}
  Unlike vector spaces, a minimal generating set need not to be independent. For example \(\{2, 3\} \subseteq \Z\) generates \(\Z\) but is not linear independent as \((-3) \cdot 2 + (2) \cdot 3 = 0\).
\end{eg}

However, like vector spaces, in case a module is freely generated, it is isomorphic to direct sums of copies of the ring:

\begin{lemma}
  If \(S = \{m_1, \dots, m_k\} \subseteq M\) freely generates \(M\) then
  \[
    M \cong R^k
  \]
  as an \(R\)-module.
\end{lemma}

\begin{proof}
  This is entirely analogous to vector spaces. Let
  \begin{align*}
    f: R^k &\to M \\
    (r_1, \dots, r_k) &\mapsto \sum_i r_im_i
  \end{align*}
  It is surjective as \(S\) generates \(M\) and injective as \(m_i\)'s are independent.
\end{proof}

If an \(R\)-module is generated by \(m_1, \dots, m_k\), we have seen before that there is a surjection \(f: R^k \surj M\). We define

\begin{definition}[Relation module]\index{relation module}
  The \emph{relation module} for the generators is
  \[
    \ker f \leq \R^k.
  \]
\end{definition}

As \(M \cong R^k/\ker f\), knowing \(M\) is equivalent to knowing the relation module.

\begin{definition}[Finitely presented]\index{finitely presented}
  \(M\) is \emph{finitely presented} if there is a finitely generating set \(m_1, \dots, m_k\) for which the associated relation module is finitely generated.
\end{definition}

Let \(\{n_1, \dots, n_r\} \subseteq \ker f \leq R^k\) be a set of generators. Then
\[
  n_i =
  \begin{pmatrix}
    r_{1i} \\
    r_{ri} \\
    \vdots \\
    r_{ki}
  \end{pmatrix}
\]
and \(M\) is generated by \(m_1, \dots, m_k\) subject to relations
\[
  \sum_{j = 1}^k r_{ij} m_j = 0
\]
for \(1 \leq i\leq r\).

\begin{proposition}[Invariance of Dimension]
  If \(R^n \cong R^m\) then \(n = m\).
\end{proposition}

\begin{note}
  This does not hold in general for modules over non-commutative rings.
\end{note}

\begin{proof}
  As a general strategy, let \(I \ideal R\). Then
  \[
    IM = \left\{\sum a_im_i: a_i \in I, m_i \in M \right\} \leq M
  \]
  is a submodule as
  \[
    r \cdot \sum a_im_i = \sum (ra_i)m_i \in IM.
  \]
  Thus we have a quotient \(R\)-module \(M/IM\). We can make this into an \((R/I)\)-module via
  \[
    (r + I) \cdot (m + IM) = rm + IM.
  \]

  Let \(I \ideal R\) be a maximal proper ideal (this requires Zorn's Lemma). Then \(R/I\) is a field and therefore \(R^n \cong R^m\) implies
  \begin{align*}
    R^n/IR^n &\cong R^m/IR^m \\
    (R/I)^n &\cong (R/I)^m
  \end{align*}
  This is a vector space isomorphism so \(n = m\).
\end{proof}

We have classified all finite abelian groups (well, at least we declared so), i.e.\ \(\Z\)-modules. What if we want to classify all \(R\)-modules? That is going to be the final goal we will build towards.

Recall that \(M\) is finitely generated by \(m_1, \dots, m_k\) if and only if there is a surjection \(f: R^k \surj M\). \(M\) is finitely presentely if and only \(\ker f\) is finitely generated, say \(n_1, \dots, n_\ell\). Let
\[
  n_i =
  \begin{pmatrix}
    r_{1i} \\
    r_{ri} \\
    \vdots \\
    r_{ki}
  \end{pmatrix}
\]
then such an \(R\)-module \(M\) is determined by the matrix
\[
  \begin{pmatrix}
    r_{11} & r_{12} & \cdots & r_{1\ell} \\
    r_{r1} & & & \\
    \vdots & & \ddots & \\
    r_{k1} & & & r_{k\ell}
  \end{pmatrix}
  \in \matrixring_{k, \ell}(R).
\]

\subsection{Matrices over Euclidean Domains}

For this section assume \(R\) to be a Euclidean domain and let \(\varphi: R \setminus \{0\} \to \Z_{\geq 0}\) be the Euclidean function. For \(a, b \in R\), we have shown that \(\gcd(a, b)\) exists and is unique up to associates. In addition, the Euclidean algorithm shows that \(\gcd(a, b) = ax + by\) for some \(x, y \in R\).

What follows would be very similar to what we have learned in IB Linear Algebra --- in fact identical except a single modification:

\begin{definition}[Elementary row operation]\index{tbd}
  \emph{Elementary row operation} on an \(m \times n\) matrix \(m\) with entries in \(R\) are
  \begin{enumerate}
  \item Add \(c \in R\) times the \(i\)th row to the \(j\)th row where \(i \neq j\). This can be realised by left multiplication by \(mI + C\) where \(C\) is \(c\) in \((i,j)\)th position and \(0\) elsewhere.
  \item Swapping the \(i\)th and \(j\)th row where \(i \neq j\). Realised by left multiplication by
  \[
    \begin{pmatrix}
      1 & 0 & \cdots & & & 0 \\
      \vdots & \ddots & & & & \vdots \\
      & & 0 & & 1 & 0 \\
      0 & \cdots & 0 & \ddots & 0 & 0 \\
      & & 1 & 0 & 0 & & \\
      \vdots & &&  \ddots & \\
      0 & & \cdots & & \cdots & 0
    \end{pmatrix}
  \]
\item Multiply the \(i\)th row by a \emph{unit} \(c \in R\). Realised by left multiplication by
  \[
    \begin{pmatrix}
      1 & 0 & \cdots & & 0 \\
       & \ddots & & & \\
      \vdots & & c & & \vdots \\
       & & & \ddots & \\
      0 & \cdots & & 0 & 1
    \end{pmatrix}
  \]
  \end{enumerate}
\end{definition}

\begin{definition}[Elementary column operation]
  Defined analogously by replacing ``row'' with ``column''.
\end{definition}

Similary to IB Linear Algebra, we define an equivalence relation

\begin{definition}[Equivalence]\index{equivalence}
  \(A, B \in \matrixring_{m, n}(R)\) are \emph{equivalent} if there is a sequence of elementary row and column operations taking \(A\) to \(B\).
\end{definition}

If \(A\) and \(B\) are equivalent then there are invertible square matrices \(P\) and \(Q\) such that
\[
  B = QAP^{-1}.
\]

\begin{theorem}[Smith Normal Form]\index{Simith Normal Form}
  An \(n \times m\) matrix over a Euclidean domain \(R\) is equivalent to
  \[
    \begin{pmatrix}
      d_1 \\
      & d_2 \\
      & & \ddots \\
      & & & d_r \\
      & & & & 0 \\
      & & & & & \ddots \\
      & & & & & & 0
    \end{pmatrix}
  \]
  where the \(d_i\)'s are non-zero and
  \[
    d_1 \divides d_2 \divides \cdots \divides d_r.
  \]
\end{theorem}

\begin{proof}
  This proof is going to be algorithmic. If \(A = 0\) we are done. Otherwise there is a \(A_{ij} \neq 0\). By swapping \(1\)st and \(i\)th row, and \(1\)st and \(i\)th column we may suppose \(A_{11} \neq 0\). We want to reduct \(\varphi(A_{11})\) as much as possible. Split into three cases:
  \begin{itemize}
  \item Case 1: if there is a \(A_{1j}\) not divisible by \(A_{11}\) then have
    \[
      A_{1j} = q A_{11} + r
    \]
    with \(\varphi(r) < \varphi(A_{11})\). Add \(-q\) times the \(1\)st column to the \(j\)th. This makes the \((1,j)\)th entry \(r\). Swap \(i\)th and \(j\)th column to get \(A_{11} = r\). Thus we have \emph{stictly} decreased the \(\varphi\) value of the \((1, 1)\) entry.
  \item Case 2: if \(A_{11}\) does not divide some \(A_{i1}\), do the analogous to entries in the first column to strictly reduce \(\varphi(A_{11})\).

    As \(\varphi(A_{11})\) can only strictly decrease finitely many times, after some applications of Case 1 and 2 we can assumes \(A_{11}\) divides all the entries in the \(1\)st row and \(1\)st column. If \(A_{1j} = q A_{11}\) then we can add \(-q\) times the \(1\)st column to the \(j\)th row to make the \((i, j)\)th entry \(0\). Thus we obtain
    \[
      A =
      \begin{pmatrix}
        d & 0 \\
        0 & C
      \end{pmatrix}
    \]
  \item Case 3: if there is an entry \(c_{ij}\) of \(C\) not divisible by \(d\), write
    \[
      c_{ij} = qd + r
    \]
    where \(\varphi(r) < \varphi(d)\). Conduct the following series of elementary operations
    \begin{align*}
      &
        \begin{pmatrix}
          d & 0 & \cdots & 0 & \cdots & 0 \\
          0 \\
          \vdots \\
          0 & & & c_{ij} \\
          \vdots \\
          0
        \end{pmatrix}
      \stackrel{\text{EC } 1}{\to}
      \begin{pmatrix}
        d & 0 & \cdots & d & \cdots & 0 \\
        0 \\
        \vdots \\
        0 & & & c_{ij} \\
        \vdots \\
        0
      \end{pmatrix}
      \\
      \stackrel{\text{ER } 1}{\to}&
      \begin{pmatrix}
        d & 0 & \cdots & d & \cdots & 0 \\
        0 \\
        \vdots \\
        -qd & & & r \\
        \vdots \\
        0
      \end{pmatrix}
      \stackrel{\text{ER } 2, \text{EC } 2}{\to}
      \begin{pmatrix}
        r & * & \cdots & * \\
        * \\
        \vdots & & * \\
        *
      \end{pmatrix}
    \end{align*}
    Repeat Case 1 and 2, we finally get
    \[
      \begin{pmatrix}
        d' \\
        & C'
      \end{pmatrix}
    \]
    where \(\varphi(d') < \varphi(d)\).
  \end{itemize}

  Eventually we can suppose that \(d'\) divides every entry of \(C'\). By induction \(C'\) is equivalent to
  \[
    \begin{pmatrix}
      d_2 \\
      & d_3 \\
      & & \ddots \\
      & & & d_r \\
      & & & & 0 \\
      & & & & & \ddots \\
      & & & & & & 0
    \end{pmatrix}
  \]
  with
  \[
    d_2 \divides d_3 \divides \cdots \divides d_r
  \]
  and we must have \(d' \divides d_i\) for \(i > 1\).
\end{proof}

\begin{remark}
  The \(d_i\)'s in Smith Normal Form are unique up to associates.
\end{remark}

Certainly Smith Normal Form is nice form and the algorithm guarantees that it exists and is unique. However, the computation is too cumbersome to be useful. However, if we could prove it is invariant under matrix conjugation, we may apply some clever tricks to extract the \(d_i\)'s in Smith Normal Form without explicitly computing them.

\begin{definition}[Minor]\index{minor}
  A \(k \times k\) \emph{minor} of a matrix \(A\) is the determinant of a matrix formed by forgetting all but \(k\) rows and \(k\) columns of \(A\).
\end{definition}

\begin{definition}[Fitting ideal]\index{Fitting ideal}
  The \(k\)th \emph{Fitting ideal} of \(A\) \(\fit_k(A) \ideal R\) is the ideal generated by all \(k \times k\) minors of \(A\).
\end{definition}

Given a matrix \(A\) in Smith Normal Form as above with \(d_1 \divides \cdots \divides d_r\), the only \(k \times k\) submatrices which do not have a whole row or column \(0\) are those which keep both \(i_1\)th row and \(i_1\)th column, both \(i_2\)th row and \(i_2\)th column, etc. Therefore
\begin{align*}
  \fit_k(A) &= \left(\det
    \begin{pmatrix}
      d_{i_1} \\
      & d_{i_2} \\
      & & \ddots \\
      & & & d_{i_k}
    \end{pmatrix}
  \right) \\
            &= (d_{i_1} \cdots d_{i_k}: \text{ sequences } i_1, \dots, i_k) \\
            &= (d_1d_2 \cdots d_k)
\end{align*}
as \(d_m \divides d_{i_m}\) for all \(m\).

Therefore from the above computation \(\fit_k(A)\) and \(\fit_{k - 1}(A)\) determine \(d_k\) up to associates.

\begin{lemma}
  If \(A\) and \(B\) are equivalent matrices then \(\fit_k(A) = \fit_k(B)\) for all \(k\).
\end{lemma}

\begin{proof}
  It amounts to show that elementary operations does not change \(\fit_k(A) \ideal R\). We do the first type of row operation. Fix a \(k \times k\) submatrix \(C\) in \(A\). Recall the this row operation adds \(\lambda\) times the \(i\)th row to the \(j\)th row. Depending on \(i\) and \(j\), split into three cases:
  \begin{itemize}
  \item Case 1: if the \(j\)th row is not in \(C\) then \(C\) is unchanged, so is its determinant.
  \item Case 2: if the \(i\)th and \(j\)th rows are both in \(C\), the operation changes \(C\) by a row operation so its determinant is unchanged.
  \item Cases 3: if the \(j\)th row is in \(C\) but the \(i\)th is not, suppose wlog the \(i\)th row of \(A\) corresponding to columns of \(C\) has entries \((f_1, f_2, \dots, f_k)\). After the row operation, \(C\) is changed to \(C'\) whose \(j\)th row is
    \[
      (c_{j, 1} + \lambda f_1, c_{j, 2} + \lambda f_2, \dots, c_{j, k} + \lambda f_k).
    \]
    By expansion along the \(j\)th row,
    \[
      \det C' = \det C \pm \lambda \det D
    \]
    where \(D\) is the matrix obtained by replacing the \(j\)th row of \(C\) with \((f_1, \dots, f_k)\), which is a \(k \times k\) submatrix of \(A\) up to reordering (which is accounted for by the \(\pm\) sign), by multilinearity of \(\det\). So \(\det C' \in \fit_k(A)\) as it is a linear combination of minors. Therefore \(\fit_k(A') \subseteq \fit_k(A)\) where \(A'\) is obtained from \(A\) by this operation. As row operations are inertible, we must have equality.
  \end{itemize}

  The other two types of row operations are similar but easier. Column operations follow analogously.
\end{proof}

\begin{eg}
  Let
  \[
    A =
    \begin{pmatrix}
      2 & -1 \\
      1 & 2
    \end{pmatrix}
    \in \matrixring_{2, 2}(\Z).
  \]
  Algorithmically, we can carry out the following sequence of operations to obtain Smith Normal Form:
  \[
    \begin{pmatrix}
      2 & -1 \\
      1 & 2
    \end{pmatrix}
    \stackrel{\text{ER } 2}{\to}
    \begin{pmatrix}
      1 & 2 \\
      2 & -1
    \end{pmatrix}
    \stackrel{\text{ER } 1}{\to}
    \begin{pmatrix}
      1 & 2 \\
      0 & -5
    \end{pmatrix}
    \stackrel{\text{ER } 1}{\to}
    \begin{pmatrix}
      1 & 0 \\
      0 & -5
    \end{pmatrix}
    \stackrel{\text{ER } 3}{\to}
    \begin{pmatrix}
      1 & 0 \\
      0 & 6
    \end{pmatrix}
  \]

  Alternatively, using what we have just proved,
  \begin{align*}
    \fit_1(A) &= (2, -1, 2, 1) = (1) \\
    \fit_2(A) &= (\det A) = (5)
  \end{align*}
  so \(d_1 = 1, d_1d_2 = 5\) so \(d_2 = 5\).
\end{eg}

Recall that we have remarked that a submodule of a finitely genereated module may not be finitely generated. However the following lemma tells us that submodules of finitely generated free modules over some particular rings are so:

\begin{lemma}
  Let \(R\) be a PID. Any submodule of \(R^n\) is generated by at most \(n\) elements.
\end{lemma}

\begin{proof}
  Let \(N \leq R^n\) and consider the ideal
  \[
    I = \{r \in R: \exists r_2, \dots, r_n \text{ such that } (r, r_2, \dots, r_n) \in N\},
  \]
  which is the image of \(N \stackrel{\iota}{\to} R^n \stackrel{\pi_1}{\to} R\), a submodule of \(R\).

    As \(R\) is a PID, \(I = (a) \ideal R\) for some \(a \in R\). Thus there is some
    \[
      n_1 = (a, a_2, a_2, \dots, a_n) \in N.
    \]
    Suppose \((r_1, r_2, \dots, r_n) \in N\). Then there exists some \(x \in R\) such that \(r_1 = ax\). Then
    \[
      (r_1, \dots, r_n) - x \cdot n_1 = (0, r_2 - xa_2, \dots, r_n - xa_n) \in N \cap (0 \oplus R^{n-1}).
    \]

    By induction \(N \cap (0 \oplus R^{n - 1}) \cong N' \leq R^{n - 1}\) is generated by \(n_2, \dots, n_n\) so \(n_1, \dots, n_n\) generate \(N\).
\end{proof}

\begin{theorem}
  Let \(R\) be a Euclidean domain and \(N \leq R^n\). Then there is a basis \(v_1, \dots, v_n\) of \(R^n\) such that \(N\) is generated by \(d_1v_1, \dots, d_rv_r\) for some \(0 \leq r \leq n\) and some \(d_1 \divides \dots \divides d_r\).
\end{theorem}

\begin{proof}
  By the previous lemma there are \(x_1, \dots, x_m \in N\) which generate \(N\) and \(0 \leq m \leq n\). Each \(x_i\) is an element of \(R^n\) so we can form an \(n \times m\) matrix whose first \(m\) columns are \(x_i\), i.e.
  \[
    A =
    \begin{pmatrix}
      \uparrow & \uparrow & & \uparrow & \uparrow & & \uparrow \\
      x_1 & x_2 & \cdots & x_m & 0 & \cdots & 0 \\
      \downarrow & \downarrow & & \downarrow & \downarrow & & \downarrow \\
    \end{pmatrix}
    \in \matrixring_{n, m}(R)
  \]

  We can put \(A\) into Smith Normal Form with diagonal entries \(d_1 \divides \cdots \divides d_r\) by elementary operations. Each row operation is given by a change of basis of \(R^n\) and each column operation is given by rechoosing the generating set \(x_1, \dots, x_m\). Thus after a change of basis of \(R^n\) to \(v_1, \dots, v_n\), \(N\) is generated by \(d_1v_1, \dots, d_rv_r\).
\end{proof}

\begin{corollary}
  A submodule \(N \leq R^n\) is isomorphic to \(R^m\) for some \(m \leq n\).
\end{corollary}

\begin{proof}
  By the theorem above, we can find a basis \(v_1, \dots, v_n\) for \(R^n\) such that \(N\) is generated by \(d_1v_1, \dots, d_mv_m\). These are linearly independent as a dependence between them would give a dependence between \(v_1, \dots, v_n\).
\end{proof}

Now we are ready for the big theorem in this course:

\begin{theorem}[Classification Theorem for Finitely Generated Modules over Euclidean Domain]\index{tbd}
  \label{thm:module over ED}
  Let \(R\) be a Euclidean domain and \(M\) a finitely generated \(R\)-modules. Then
  \[
    M \cong \frac{R}{(d_1)} \oplus \frac{R}{(d_2)} \oplus \dots \oplus \frac{R}{(d_r)} \oplus R \oplus \dots \oplus R
  \]
  for some \(d_i \neq 0\) with \(d_1 \divides d_2 \divides \cdots \divides d_r\).
\end{theorem}

\begin{proof}
  Let \(M\) be generateed by \(m_1, \dots, m_n \in M\), giving a surjection \(\varphi: R^n \surj M\) so \(M \cong R^n/\ker \varphi\). By the previous theorem there is a basis \(v_1, \dots, v_n\) of \(R^n\) such that \(\ker \varphi\) is generated by \(d_1v_1, \dots, d_rv_r\) with \(d_1 \divides \cdots \divides d_r\). Thus by changing the basis of \(R^n\) to \(v_i\)'s, \(\ker \varphi\) is generated by columns of
  \[
    \begin{pmatrix}
      d_1 \\
      & d_2 \\
      & & \ddots \\
      & & & d_r \\
      & & & & 0 \\
      & & & & & \ddots \\
      & & & & & & 0
    \end{pmatrix}
  \]
  so
  \[
    M \cong \frac{R^n}{\ker \varphi} \cong \left( \bigoplus_{i = 1}^r \frac{R}{(d_i)} \right) \oplus R \oplus \dots \oplus R
  \]
  as required.
\end{proof}

\begin{eg}
  Let \(R = \Z\), a Euclidean domain, and \(A\) be the abelian group (i.e.\ \(\Z\)-module) generated by \(a, b, c\), subject to
  \[
    \begin{cases}
      2a + 3b + c = 0 \\
      a + 2b = 0 \\
      5a + 6b + 7c = 0
    \end{cases}
  \]
  Thus \(A = \Z^3/N\) where \(N \leq \Z^3\) is generated by \((2, 3, 1)^T, (1, 2, 0)^T, (5, 6, 7)^T\). The matrix \(A\) whose columns are these vectors
  \[
    A =
    \begin{pmatrix}
      2 & 1 & 5 \\
      3 & 2 & 6 \\
      1 & 0 & 7
    \end{pmatrix}
  \]
  has Smith Normal Form with diagonal entries \(1, 1, 3\):

  \begin{proof}
    \begin{align*}
      \fit_1(A) &= (1) \\
      \fit_2(A) &\supseteq \left( \det
                  \begin{pmatrix}
                    2 & 1 \\
                    3 & 2
                  \end{pmatrix}
                        \right)
                        = (1) \\
      \fit_3(A) &= (\det A) = 3
    \end{align*}
    so \(d_1 = 1, d_1d_2 = 1, d_1d_2d_3 = 3\).
  \end{proof}

  After change of basis, \(N\) is generated by \((1, 0, 0)^T, (0, 1, 0)^T, (0, 0, 3)^T\) so
  \[
    A \cong \Z/1\Z \oplus \Z/1\Z \oplus \Z/3\Z \cong \Z/3\Z.
  \]
\end{eg}

We can derive, as a corollary actually, what we stated earlier without proof

\begin{theorem}[Structure Theorem for Finitely Generated Abelian Groups]
  Any finitely generated abelian group is isomorphis to
  \[
    C_{d_1} \times C_{d_2} \times \dots \times C_{d_r} \times C_\infty \times \dots \times C_\infty
  \]
  with \(d_1 \divides \cdots \divides d_r\).
\end{theorem}

\begin{proof}
  ``Trivial'' should suffice here but let us spell it out: apply \nameref{thm:module over ED} to \(\Z\), and note that
  \[
    \Z/(d) = C_d, \, \Z = C_\infty.
  \]
\end{proof}

The above classification theorem decompose into modules whose relation modules' principal ideals form a descending chain by divisibility. It turns out it is also possible to decompose by the coprime factors of the relation modules. Before that let us prove something we have known for a (very) long time, but at a high level:

\begin{lemma}[Chinese Remainder Theorem]
  Let \(R\) be a Euclidean domain and \(a, b \in R\) with \(\gcd(a, b) = 1\). Then
  \[
    R/(ab) \cong R(a) \oplus R/(b).
  \]
\end{lemma}

\begin{proof}
  Consider the \(R\)-module homomorphism
  \begin{align*}
    \varphi: R/(a) \oplus R/(b) &\to R/(ab) \\
    (r_1 + (a), r_2 + (b)) &\mapsto br_1 + ar_2 + (ab)
  \end{align*}
  As \(\gcd(a, b) = 1\), \((a, b) = (1)\) so \(1 = xa + yb\) for some \(x, y \in R\). Therefore for \(r \in R\), \(r = rxa + ryb\) so
  \[
    r + (ab) = rxa + ryb + (ab) = \varphi((ry + (a), rx + (b)))
  \]
  and so \(\varphi\) is surjective.

  If \(\varphi((r_1 + (a), r_2 + (b))) = 0\) then \(br_1 + ar_2 \in (ab)\). Thus \(a \divides br_1 + ar_2, a \divides br_1\). As \(\gcd(a, b) = 1\), \(a \divides r_1\) so \(r_1 + (a) = 0 + (a)\). Similarly \(r_2 + (b) = 0 + (b)\) so \(\varphi\) is injective.
\end{proof}

We thus have

\begin{theorem}[Primary Decomposition Theorem]
  Let \(R\) be a Euclidean domain and \(M\) be a finitely generated \(R\)-module. Then
  \[
    M \cong \bigoplus_{i = 1}^n N_i
  \]
  with each \(N_i\) either equal to \(R\) or \(R/(p^m)\) for some prime \(p \in R\) and \(n \geq 1\).
\end{theorem}

\begin{proof}
  Note that if \(d = p_1^{m_1} \cdots p_k^{m_k}\) with \(p_i \in R\) distinct primes, by the previous lemma
  \[
    \frac{R}{(D)} \cong \frac{R}{(p_1^{n_1})} \oplus \dots \oplus \frac{R}{(p_k^{m_k})}.
  \]
  Plug this into \nameref{thm:module over ED} to get the required result.
\end{proof}

\subsection{\texorpdfstring{\(\F[X]\)}{ùîΩ}-modules and Normal Form}

For any field \(\F\), \(\F[X]\) is a Euclidean domain and so the results of the last section apply. If \(V\) is an \(\F\)-vector space and \(\alpha: V t\ V\) is an endomorphism, then we have
\begin{align*}
  \F[X] \times V &\to V \\
  (f, v) &\mapsto f(\alpha)(v)
\end{align*}
which makes \(V\) into an \(\F[X]\)-module, call it \(V_\alpha\). It turns out that \(\F[X]\)-module is the correct tool to study endomorphisms and many results in IB Linear Algebra, as well as many further results in algebra, can be obtained by looking into the \(\F[X]\)-module structure.

\begin{lemma}
  If \(V\) is finite-dimensional then \(V_\alpha\) is finitely generated as an \(\F[X]\)-module.
\end{lemma}

\begin{proof}
  \(V\) is a finitely generated \(\F\)-module and \(\F \leq \F[X]\) so \(V\) is also a finitely generated \(\F[X]\)-module.
\end{proof}

\begin{eg}\leavevmode
  \begin{enumerate}
  \item Suppose \(V_\alpha \cong \F[X]/(X^r)\) as an \(\F[X]\)-module. This has \(\F\)-basis \(\{X^i\}_{i = 0}^{r - 1}\) and the action 
  \end{enumerate}
\end{eg}
\printindex
\end{document}
